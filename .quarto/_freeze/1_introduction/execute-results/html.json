{
  "hash": "2b9feab5ebe3744a726b1e6f065a60ac",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduction\n\nAdrien-Marie Legendre was a French mathematician, who in the early 19th century\ntook a particular interest in predicting the trajectories of comets. The\nunderstanding of the movements of celestial objects represented one of the main\napplications of mathematics at that time. There were great scientific challenges\nin astronomy, such as the stability of the solar system, but there were also\nmore earthbound applications, such as celestial navigation. Legendre wanted to\ndescribe the trajectory of a comet by a parabolic path, and he wanted to use\nobservations of points from a comet's trajectory to infer the best path\ncoefficients. When Legendre substituted the observed points into the path\nequation he was left with more linear equations in the unknown coefficients than\nthe number of coefficients. There was no solution; the observations did not fall\nexactly on any parabolic path.\n\nLegendre was not the first to deal with such a *path fitting* problem, and\nvarious techniques had been developed, none of which were completely\nsatisfactory. One solution was to disregard some equations so that a unique\nsolution could be found to the remaining. The resulting path would then match\nsome observed points exactly. But which equations should be disregarded? And\nwould the resulting path be a good approximation to the omitted observation\npoints?\n\nLegendre's parabolic paths could only produce an approximation of the\nobservations and the future trajectory of the comet. It would not be an exact\nmatch. The deviations -- or errors -- from predicted trajectories of celestial\nobjects were primarily due to unsystematic inaccuracies in the measurements. For\nLegendre and his contemporaries there was no clear solution as to how deviations\nfrom a path should be described and handled. The statistical theory was in its\ninfancy, and the numerical computations required to obtain a descriptive\nstatistical understanding of their nature was very labor intensive. Today it is\nwell known that such deviations can be described statistically, and the methods\nof path or curve fitting are beneficially studied using statistical theory.\n\nLegendre found an elegant and applicable algebraic solution to his path fitting\nproblem -- as described in the section below -- which was quickly adapted by\nastronomers and geodesists in France and Germany. He did, however, not realize\nthe statistical nature of his method. This aspect was developed and expanded by\nothers in the 19th and 20th century, and the theory evolved into the statistical\nsubject we know today as *regression*.\n\nRegression is one of the core pillars of statistics, and the statistical theory\nof regression is a theory about models and methods for relating observables, for\nexample, the coordinates on the path of a comet. The general idea of relating\nobservables through a combination of a systematic and an unsystematic component\npenetrates most areas of science, technology and business today. The systematic\ncomponent represents a predictable relation between observables, while the\nunsystematic component is statistical of nature. It may represent measurement\nerrors, sampling inaccuracies, lack of information or other mechanisms or\ncombinations of mechanisms whose contribution cannot be captured by the\nsystematic component. It is a theory whose premise is that the unsystematic\ncomponent is statistical. It is a theory whose primary objectives are to devise\nmethods for fitting models to data and to investigate properties of such\nmethods.\n\n## A brief history of regression\n\nWe give in this section a historical perspective on regression. Tracking the\norigin of regression methods and its terminology provides a historical context\nfor the theory developed in this book. It is a multifaceted story, and the\nintroduction will provide a birds eye perspective on what this book is about.\n\n### Linear regression\n\nLegendre's book [@Legendre:1805] on the determination of the orbit of comets was\npublished in 1805 containing the appendix *Sur la Méthode des moindres carrés*.\nIt outlined a method for fitting a linear equation with unknown coefficients to\nobservations. The English translation of the title of the appendix is *On the\nMethod of least squares*, and it is the earliest known appearance of the method\nin print. Independently of Legendre, Carl F. Gauss [@Gauss:1809] invented\nthe Gaussian distribution and used it to derive the method of least squares from\nLaplace's principle of inverse probability. Gauss was like Legendre motivated by\nproblems in astronomy -- specifically the description and prediction of the\nmotions of planets. The invention of the method of least squares marks a turning\npoint in the history of regression. Earlier work by Pierre-Simon Laplace, and\nrelated work by Roger Joseph Boscovich with applications to geodesy, using the\nmethod of least absolute deviation, had been less successful. The method of\nleast absolute deviation leads to a nonlinear estimator that is hard to compute.\nBy contrast, the least squares estimator is a linear estimator that solves the\n(linear) normal equation, and Gauss presented a useful method for its numerical\ncomputation. The generality of the linear model framework considered by Gauss is\nimpressive and corresponds essentially to that treated in Chapter\n[\\[chap:lm\\]](#chap:lm){reference-type=\"ref\" reference=\"chap:lm\"} in this book.\n\nIn the years after the invention of the method of least squares, Gauss and\nLaplace derived several theoretical results about the method including sampling\nproperties of the least squares estimator and confidence intervals for the\nparameters. The logical foundation of the method of least squares was, however,\nconfusing at the time. Gauss's initial derivation was -- as mentioned above --\nbased on Laplace's principle of inverse probability. Later, Laplace as well as\nGauss sought justification of the method in the sampling properties of the\nresulting estimator. Specifically by investigating if the estimator has a form\nof minimal estimation error. Gauss showed what is today known as the\nGauss-Markov theorem: the least squares estimator has minimal variance among all\nlinear unbiased estimators.\n\nThe models that Gauss fitted to data using his method of least squares were not\nknown to him as regression models. That terminology originates from Sir Francis\nGalton [@Galton:1886]. In studies of hereditary traits Galton observed an\napparent regression (in the sense of a movement) towards a mediocre state in the\noffspring compared to the parent(s). Originally he considered the size of plant\nseeds and later the height of human individuals. What Galton had discovered was\nthe phenomenon known today as regression toward the mean. He explained the\nphenomenon statistically. The trait is partly inherited from the parent and\npartly a consequence of additional variation. \nThis results in a bivariate\ndistribution showing a co-relation of the trait in parent and offspring.\nThough Galton correctly described the phenomenon statistically, he wrongly \nexplained the regression phenomenon as being caused by inheritance from the\nancestry of the parent in addition to the parent themselves. \n\nTo understand the co-relation Galton observed in his heriditary studies, he contributed to the theory of the bivariate\nnormal distribution and introduced the general notion of correlation. The\ntechnical details of what is known today as the *Pearson* correlation\ncoefficient was worked out by Karl Pearson, though. Galton nevertheless \nrealized the usefulness of\nregression and correlation in various application areas, and the regression\nterminology was adapted by others, notably Karl Pearson. Interestingly, the\ninitial developments of regression analysis in British mathematical statistics\nwere independent of the continental literature, and the regression models were,\nin particular, not framed in the linear model terminology of Gauss.\n\nIn the first half of the 20th century, Sir Ronald A. Fisher had an enormous\nimpact on mathematical statistics. He developed Analysis of Variance (ANOVA),\nintroduced the likelihood concept and advocated the general principle of maximum\nlikelihood estimation among other things. The primary application of ANOVA was\nto designed experiments [@Fisher:1935], and though far from Gauss's applications on the\nmodeling of the motion of celestial objects, the ANOVA models fit into the\nframework of the linear model. \n\nFisher did not rely on or acknowledge the\ncontributions of his predecessors to any great extent, and he rediscovered known\nresults about the method of least squares. But he also contributed significantly\nto the theory, for instance by deriving sampling distributions of test\nstatistics, notably the $F$-test statistic. His presentation and applications of\nANOVA models have been highly influential on the current terminology, which is,\nfor instance, reflected in the `anova()` function in R for testing linear\nhypotheses within the linear model. Subsequent developments with contributions\nfrom many people have turned the linear model into a unified and general\nstatistical theory as outlined in one of the first unifying textbooks on the\nsubject by Shayle Robert Searle [@Searle:1971]. \n\nThe history of regression is intertwined with the general history of statistics,\nand for the history before 1935 the work by Anders Hald [@Hald:2007]\nis an indispensible source. For the more recent, and much richer, development, \nof nonlinear regresssion we give a brief outline of the history \nfocused on a few selected original sources.  \n\n### Beyond linear regression\n\nRegression models, methods and applications proliferated in the second half of\nthe 20th century. Computers played a pivotal role, first of all by easing the\nsolution of nonlinear estimating equations by iterative methods, which paved the\nway for applications of nonlinear regression models and nonlinear estimation\nmethods. The use of computer simulations and Markov Chain Monte Carlo (MCMC), in\nparticular, also made Bayesian regression modeling a practical possibility. In\nthis brief account of the history, it is impossible to do justice to all the\ndifferent directions, and we will therefore narrow the focus to those regression\nmodels that are treated in this book.\n\nNot all types of regression problems fit into the linear modeling framework, and\none of the early applications of a nonlinear regression model was to the\nmodeling of mortality as a function of the dosage of a toxic agent. \nChester I. Bliss [@Bliss:1935] proposed a transformation method where dosage is log-transformed\nand death frequency is transformed by the inverse distribution function for the\nnormal distribution with mean 5 and unit variance. The resulting transformed\ndeath frequencies were called probits. The transformed data should fall on a\nstraight line, and Bliss suggested using the method of weighted least squares\nfor fitting such a line. \n\n@Bliss:1934 presented a similar idea, one year earlier, with the probits \ncomputed from an S-shaped curve inspired by, \nbut not identical to, the distribution function for the normal distribution. \nHis 1935-paper had, however, an important appendix written by Fisher, \nwhich showed how to appropriately deal with dosages for which none or \nall of the organisms survied. In this appendix, Fisher outlined how the\nmethod of maximum-likelihood could be used for estimation. In doing so he came\nup with a correction step in the weighted least squares fit, which is what we\ntoday know as the Fisher scoring algorithm.\n\nThe probit model introduced by Bliss is one among a number of nonlinear\nregression models -- notably models of counts -- that share a similar structure.\nThese regression models were unified by John Nelder and Robert Wedderburn\n[@Nelder:1972] in the framework they called generalized linear models, which\nincludes the linear model as a special case but extends it considerably. Nelder\nand Wedderburn showed that the maximum likelihood estimator in a generalized\nlinear model can be computed iteratively by the Fisher scoring algorithm. Each\nstep in this algorithm is the solution of a weighted least squares problem, and\nthey called it the Iterative Weighted Least Squares (IWLS) algorithm. They also\nparaphrased some terminology and methodology from linear models, in particular\nthe use of analysis of deviance as a generalization of ANOVA. As a curious\ndetail, Nelder and Wedderburn outlined in their original paper how generalized\nlinear models could form a useful basis for courses in statistics, which would\nunify the treatment of a number of statistical models that had previously been\ntreated separately. The theory of generalized linear models was further\ndeveloped in the 70s and 80s, and an authoritative reference on the by now\nclassical theory is the book by McCullagh and Nelder [@McCullagh:1989]. In\naddition to his theoretical contributions, Nelder also initially chaired the\nGLIM Working Party, who developed the statistical software program GLIM\n(Generalized Linear Interactive Modeling). This program contributed to making\ngeneralized linear models applicable in practice, and it was a source of\ninspiration for the later implementations [@Hastie:1991] in S and R.\n\nAnother type of regression problem that does not fit directly into the linear\nmodeling framework either is the modeling of survival times. As for generalized linear models\nthere is a question of modeling scale; at which scale is survival time most\nappropriately associated to other variables? In addition to this there is the\npractical problem that observations may be right censored. Survival models and\nestimation of life tables have a long history, in particular in actuarial\nscience and medical statistics, but a systematic approach to regression survival\nmodels was not made until the seminal contributions by Sir David R. Cox\n[@Cox:1972]. He introduced the semiparametric proportional hazards model, which\nhas since become the most widely used survival regression model. Cox made two\nmajor contributions in his paper. He introduced what he later called the partial\nlikelihood, which provided a means for deriving a sensible estimating equation,\nand he presented approximate sampling distributions of the resulting estimator.\n\nThe paper by Cox inspired a rapid development in the theory of survival\nregression models, which clarified some of his heuristic arguments and extended\nhis model class considerably. Central to this development was the use of methods\nfrom the theory of stochastic processes, in particular continuous time\nmartingale theory. Modern survival analysis theory is heavily based on the use\nof stochastic processes, and an authoritative account of this theory is the by\nnow classical book *Statistical Models Based on Counting Processes*\n[@AndersenBorganGillKeiding:1993].\n\nOn the practical side, the estimating equation derived from Cox's partial\nlikelihood in the semiparametric framework is nonlinear -- and so are likelihood\nbased estimating equations derived in a parametric framework. The estimator has\nto be computed by iterative methods. It turns out that the practical computations \ncan be carried out using the framework of generalized linear models, and the\nGLIM program could be used to fit the proportional hazards models in Cox's\nsemiparametric framework as well as for several parametric models. Though this is \ntoday mostly a curious observation, it was important for the early applicability \nof these models and methods. Later,\nspecialized software implementations were developed, such as the survival package\nfor Splus by Terry Therneau [@Therneau:2000]. The package was ported to R, and it is\ntoday the classical core package for survival analysis with R.\n\n### Regression, machine learning and computing\n\nThere were notable developments outside of mainstream statistics related to\nregression models, but little interaction between the different scientific\ncommunities. Frank Rosenblatt [@Rosenblatt:1962] explored perceptrons and\nartificial neural networks for binary classification from multiple input\nvariables. Support vector machines [@Cortes:1995] were developed for similar\npurposes, with the conceptual framework established in 1965 by Valdimir Vapnik.\n\nArtificial neural networks and support vector machines are today core subjects\nin the field of machine learning. Both can be understood as regression models of\na binary variable, and both can be modified to models of a continuous variable.\nA support vector machine is, however, not a probabilistic model, and neither\nmodel gives a simple interpretable relation between the input variables and the\ndistribution of the binary variable. The developments in statistics were\ndominated by the interaction with other scientific fields, where explanations,\ninterpretations and answers to inferential questions were sought. Inferential\nquestions that are central to statistics did not appeal to the young machine\nlearning community, whose focus was on classification performance. \n\nIn 2001 Leo\nBreiman [@Breiman:2001] divided the data modeling communities into \"two\ncultures\": those using stochastic (or probabilistic) data models and those using\nalgorithmic models. Breiman ascribed the developments of algorithmic modeling\nlargely to the machine learning community, and he strongly criticized the\nconventional statistical community for their (simplistic) probabilistic data\nmodeling. Undoubtedly, contemporary regression modeling has benefited greatly\nfrom developments in machine learning, and the \"two cultures\" interact more\nclosely today than ever before.\n\nThe development of lasso [@Tibshirani:1996] is an interesting early example. \nWhile initially developed for the linear regression model, the lasso \nesimator is nonlinear and biased -- as opposed to the least squares estimator.\n\nStatistical Learning Citation\n\n\n\n\\emph{non}linear and \\emph{biased} \nestimators, such as lasso\\cite{Tibshirani:1996}, that have allowed us to tackle \nmodern large scale data analysis\n\n\nThis brief historical account concludes by a summary of a few more recent\ninfluential developments. The practical data analysis with regression models has\nfirst of all been transformed completely by the increase of computer power and\nthe developments of statistical programming environments such as R. The computer\nserves first of all the data analyst, and interactive and exploratory data\nanalysis in the spirit of Tukey [@Tukey:1977] is now in routine use. The\ncomputer has at the same time become the lab bench of statistics, and it has\nbrought an experimental component to statistics. Most theoretical results are\ntoday scrutinized via simulation studies, which can inform us about the\napplicability and limitations of the theory. Developments in nonparametric and\nresampling based statistical methods have also made data analysis possible in\nsituations where we lack theoretical methods, where we want to impose minimal\nassumptions or where model assumptions are violated. Lastly, applications to\nlarge scale data modeling has spurred a substantial development of new\nregression modeling methods and it continues to provide exciting challenges.\n\nThe outline above of the historical developments in regression modeling contains\nthree recurrent components. These are 1) models, 2) estimation methodology and\n3) sampling properties. The first component -- the model development -- was\ndriven by applications, and the second component was developed to compute\nestimates of model parameters from data. Thus over the course of history, a\nsupply of models, such as the linear and generalized linear models, have been\ndeveloped together with methods or principles for parameter estimation, such as\nthe methods of least squares or maximum likelihood. The third component -- the\nsampling properties -- was developed to provide frequentistic quantification of\nuncertainty. There is a fourth component not mentioned above, which is model\ncritique. From Gauss to Cox there has been concern about whether the model\nassumptions were fulfilled in applications, and techniques of more or less\nformal character have been used throughout history for validating that model\nassumptions are plausible.\n\nThe four components: models; estimation methodology; sampling properties; and\nmodel critique, are the recurring components in this book as they are in\nhistory. The intention of this book is to teach the reader a modern perspective\non these cornerstones of statistics in the framework of regression modeling.\n\n## Regression modeling in this book\n\nThis book is on *descriptive and predictive regression modeling*. A regression\nmodel relates the distribution of a *response* variable to one or more\n*predictor variables*. All models considered are direct models of the\nconditional distribution of the response given the predictors, and a main\npurpose of such models is to be predictive of the response conditionally on\nobservations of the predictors. There is no claim that this is the only purpose\nof modeling in general, but it is arguably important. The book treats linear\nmodels, generalized linear models and survival regression models. They are\nwidely used in practice and obligatory parts of an education in statistics. At\nthe same time they serve as good examples when developing general statistical\nprinciples and methodologies such as likelihood based methods. The intention of\nthis book is to bridge the gap between a mathematical treatment of the model\nclasses considered and their practical use.\n\nIn predictive modeling it is fairly clear what makes a good model. Given a\nmethod for quantification of predictive accuracy, the best model is the most\naccurate model. The accuracy of a prediction is typically quantified by a *loss*\nfunction, which actually quantifies how *inaccurate* a prediction is. The\nsmaller the loss is the more accurate is the prediction. The expected loss\nquantifies how accurate the predictive model is on average. The specification of\na relevant loss function is, perhaps, not always easy. A good loss function\nshould ideally reflect the consequences of wrong predictions. On the other hand\nthere exists a selection of useful, reasonable and convenient standard loss\nfunctions that can cope with many situations of practical interest. Examples\ninclude (weighted) squared error loss, the 0-1-loss and the negative\nlog-likelihood loss. The book will not plunge into any elaborate discussions of\nthe choice of loss but focus on how to fit models to data with standard choices.\n\nOne of the biggest challenges in practice is to select and fit a model to a data\nset in such a way that it will preserve its predictive accuracy when applied to\nnew independent data. The model should be able to *generalize* well to cases not\nused for the model fitting and selection process. Otherwise the model has been\noverfitted to the data, and this is to be avoided. The book will develop a range\nof methods for fitting models to data, for assessing the model fit and for\nmaking appropriate model selections while avoiding overfitting.\n\nIt is important to understand that descriptive or predictive regression models\ndo not in themselves provide information about *causal* relations between the\nresponse and the predictors. Such information can only be obtained if either the\ndata comes from a randomized study, or if we are willing to make untestable\nassumptions. In the latter case, this will involve a thorough understanding of\nthe subject matter field (which assumptions can we make?). Causal modeling is\nnot just a technical question about distributional assumptions, but a structural\nmodeling question that the data cannot reveal by themselves. It goes deeper than\npredictive modeling, and the techniques needed for causal modeling go beyond\nwhat is treated in this book. Typically they will involve some modeling of the\ndistribution of the predictor variables as well.\n\nWith such words of warning about not making causal interpretations of predictive\nmodels, we should remind ourselves of the usefulness of predictive models. They\nare invaluable in automatized processes like spam filters or image and voice\nrecognition. They make substantial contributions to medical diagnosis and\nprognosis, to business intelligence, to prediction of customer behavior, to risk\nprediction in insurance companies, pension funds and banks, to weather forecasts\nand to many other areas where it is of interest to know what we cannot (yet)\nobserve.\n\n## Organization\n\nThe book consists of a combination of theoretical chapters and larger case\nstudies. Instead of providing a lot of small, simple and somewhat artificial\ndata examples to illustrate a point, the relevant points are illustrated by\nlarger real case studies. The hope is that this will ease the transition from\ntheory to practice. The price to pay is that there are some distractions in\nforms of real data problems. Data rarely behaves well. There are missing values\nand outliers, the models do not fit the data perfectly, the data comes with a\nstrange encoding of variables and many other issues. Issues that require\ndecisions to be made and issues on which many textbooks on statistical theory\nare silent.\n\nBy working through the case studies in detail it is the hope that many relevant\npractical problems are illustrated and appropriate solutions are given in such a\nway that the reader is better prepared to turn the theory into applications on\nher own.\n\nThe theory chapters focus, on the other hand, on presenting the mathematical\nframework for regression modeling together with the relevant mathematical theory\nsuch as derivations of likelihood functions, likelihood estimators, estimation\nalgorithms and properties of likelihood estimators. Other chapters focus on more\ngeneral methodological questions. This includes developing methods for the\ncomplex decision process that practical data analysis is.\n\n## Using R\n\nWe use the programming language R[^1_introduction-1] throughout to illustrate\nhow good modeling strategies can be carried out in practice on real data, but \nthe book will not provide an introduction to the R language. Consult the R\nmanuals [@R:2013] or the many textbooks on R programming and data analysis \n[@Hadley:2019;@Hadley:2023]. The classical and influential book\n*Modern Applied Statistics with S* [@Venables:2002] and the\ncorresponding MASS package (comes with the R distribution) should \nalso be mentioned. Many\nclassical statistical models and methods are supported by the MASS package\nand documented in the book. \n\n[^1_introduction-1]: <https://www.r-project.org/>\n\nThe case studies in this book are complete with R code that covers all aspects\nof the analysis. They represent an integration of data analysis with \ndocumentation and reporting. This is an adaptation to data analysis of *literate\nprogramming* [@knuthLiterateProgramming1984]. The main idea is that the writing\nof a report that documents the data analysis and the actual data analysis are\nmerged into one document. This supports the creation of *reproducible analysis*,\nwhich is a prerequisite for reproducible research. To achieve this integration\nthe book was written using the Quarto[^1_introduction-2] publishing system and\nthe R package knitr[^1_introduction-3]. Quarto and knitr allows you to integrate\ntext with chunks of R code and produce an output document in various formats,\ne.g., a webpage or a pdf document, that include text, code and output from\nevaluating the code. The use of the RStudio[^1_introduction-4] or\nPositron[^1_introduction-5] integrated development environments (IDE) is also\nrecommended. Both are developed by Posit PBC as open source projecs and the\ndesktop versions are free to use.\n\n[^1_introduction-2]: <https://quarto.org/>\n\n[^1_introduction-3]: <https://yihui.name/knitr/>\n\n[^1_introduction-4]: <https://posit.co/download/rstudio-desktop/>\n\n[^1_introduction-5]: <https://positron.posit.co/>\n\nACCESS TO renv ENVIRONMENT INFORMATION FOR RUNNING ALL CODE.\n\nTHE RwR package to distribute data and other stuff. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2) ## Grammar of graphics\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}