{
  "hash": "1cf05acb554048d40c318d8e5b08d896",
  "result": {
    "engine": "knitr",
    "markdown": "\n::: {.cell}\n\n:::\n\n\n# The linear model {#sec-linear}\n\n::: {.callout-warning}\nThis chapter is still a draft and minor changes are made without notice.\n:::\n\nThe linear model is the classical workhorse of regression modeling. It is a\ntractable model with many desirable practical and theoretical properties. In\ncombination with nonlinear variable transformations and basis expansions it can\nbe surprisingly flexible, and even if it is not the optimal model choice in\na given situation, it can work well as a benchmark or fallback model whenever\nmore sophisticated models are considered. \n\n## Outline and prerequisites \n\nThis chapter introduces the linear model as well as aspects of the general regression modeling \nframework and terminology used throughout book. An example on fire insurance claim size modeling \nis introduced in @sec-claim-sizes and used throughout the chapter to illustrate the theory. \n@sec-estimation-theory covers how the linear model is fitted to data,\nand @sec-sampling-distributions gives results on the sampling distributions of\nparameter estimators and test statistics that are used in relation to the linear\nmodel. @sec-model-assessment and @sec-claim-size-models treat model \nassessment and comparison using the claim size models to illustrate the theory. \nThe chapter concludes in @sec-nonlinear-expansions by introducing basis expansion \ntechniques to capture nonlinear relations within the framework of the linear model.\nThe techniques are illustrated by a spline basis expansion of the relation \nbetween claim size and insurance sum in the fire insurance example.   \n\nThe chapter primarily relies on the following four R packages. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(broom)\nlibrary(splines)\n```\n:::\n\n\nA few functions from additional packages are used, but in those cases the package is made explicit\nvia the `::` operator.\n\n## Claim sizes {#sec-claim-sizes}\n\nIn this section we take a first look at a dataset from a Danish insurance\ncompany with the purpose of studying the relation between the insurance sum and\nthe claim size for commercial fire claims. The data is included as the `claims`\ndataset in the RwR package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\ndata(claims, package = \"RwR\")\n```\n:::\n\n\nWe will use this dataset to build simple linear regression models that can \npredict potential claim sizes given knowledge of the insurance sums. More\nrefined predictions as well as the possibility to differentiate policy prices\ncan also be achieved by breaking down such a relation according to other\nvariables.\n\n\n::: {.cell .column-margin .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='First six rows of the claims data.'}\n\n```{.r .cell-code  code-fold=\"false\"}\ntibble(claims) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 3\n  claims        sum grp  \n   <dbl>      <dbl> <fct>\n1  3853.  34570294. 1    \n2  1194.   7776469. 1    \n3  3917.  26343305. 1    \n4  1259.   5502915. 1    \n5 11594. 711453346. 1    \n6 33535. 716609368. 1    \n```\n\n\n:::\n:::\n\n\nTo inspect the data we can print out a small part of\nthe dataset using the `head()` function. There is a similar `tail()` function that \nprints the last part of the dataset.\n\nThe first column printed shows the row names (in this case just the row numbers). \nThe other columns show the values of the variables in the date set. \nIn this case the dataset contains the following three variables:\n\n-  `claims` -- the claim size in Danish kroners (DKK)\n-  `sum` -- the insurance sum in Danish kroners (DKK)\n-  `grp` -- the trade group of the business\n\n\nThe `claims` and `sum` variables are the numeric, while \nthe `grp` variable is a categorical trade group. It is\nrepresented using integers but encoded and stored as a factor.\nThe trade group encoding is: \n\n- 1 = residences\n- 2 = ordinary industry\n- 3 = hotel and restaurant\n- 4 = production industry\n\n\n::: {.cell .column-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Histograms of claim size and insurance sum on log-scale.](2_linear_files/figure-html/fig-claims-histograms-1.png){#fig-claims-histograms fig-align='center' width=384}\n:::\n:::\n\n\nThe marginal distributions of claim size and insurance sum are both highly skewed and span\nseveral orders of magnitude -- as is revealed by the histograms in @fig-claims-histograms. \nFor this reason we will log-transform both\nvariables in the subsequent figures. In this section this is purely for the\npurpose of better visualization. In later sections we will consider regression\nmodels for the log-transformed variables as well. Skewness of the marginal\ndistributions does not automatically imply that the variables should be\nlog-transformed for the regression model, and we will return to this discussion\nlater.\n\nThroughout we will avoid constructing new variables in the dataset that\nrepresent transformations of other variables. We will instead use the\nfunctionalities in R for changing the scale in a plot or for transforming one or\nmore variables directly when fitting a model. The scatter plot in \n@fig-claims-scatter of claim size against insurance sum on a log-log-scale\nwas constructed using `ggplot()` from the R package ggplot2.\n\n\n::: {.cell .caption-margin layout-align=\"center\"}\n\n```{.r .cell-code}\np0 <- ggplot(\n        data = claims, \n        aes(sum, claims), \n        alpha = I(0.2)\n    ) + \n    scale_x_log10(\n        \"Insurance sum (DKK)\", \n        breaks = 10^c(6, 7, 8), \n        labels = c(\"1M\", \"10M\", \"100M\")\n        ) +\n    scale_y_log10(\n        \"Claim size (DKK)\", \n        breaks = 10^c(2, 4, 6, 8), \n        labels = c(\"100\", \"10K\", \"1M\", \"100M\")\n    ) +\n    geom_point()\n\np <- p0 + \n    geom_smooth(\n        method = \"lm\", \n        linewidth = 1, \n        se = FALSE\n    ) \np\n```\n\n::: {.cell-output-display}\n![Scatter plot of claim size against insurance sum. Note the log-log axes.](2_linear_files/figure-html/fig-claims-scatter-1.png){#fig-claims-scatter fig-align='center' width=672}\n:::\n:::\n\n\nThe code used to generate the plot shows that the plot is first\nstored as an R object named `p` and then plotted by\n\"printing\" `p` (the line just containing `p`). This is useful\nfor later reuse of the same plot with some modifications as will be illustrated\nbelow. Note that the regression line was added to the plot using the\n`geom_smooth()` function with `method = \"lm\"`.\n\nFrom @fig-claims-scatter it appears that there is a positive\ncorrelation between the insurance sum and the claim size -- the larger the\ninsurance sum is, the larger is the claim size. Note the \"line-up\" of the\nextreme claim sizes, which (approximately) fall on the line with slope 1 and\nintercept 0. These claims are capped at the insurance sum, and we should pay\nsome attention to the effect of these extremes in subsequent modeling steps.\n\nThe questions that we will focus on in this chapter are whether the trade group\nshould be included into the model and whether a linear relation (on the\nlog-log-scale) between the claim size and insurance sum is appropriate. As a\nfirst attempt to address the first question we stratify the scatter plot\naccording to the trade group.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\np + facet_wrap(~ grp, ncol = 4)\n```\n\n::: {.cell-output-display}\n![Scatter plots of claim size against insurance sum stratified according to trade group. A regression line is added to the plots for each group.](2_linear_files/figure-html/fig-claims-scatter-strat-1.png){#fig-claims-scatter-strat fig-align='center' width=100%}\n:::\n:::\n\n\n@fig-claims-scatter-strat shows that there might be some differences,\nbut it is difficult to tell from the figure alone if these differences are due\nto random variation or if they actually represent real differences between\nthe groups.\n\nThe following sections will introduce the mathematical framework for fitting\nlinear regression models and for investigating the models. Based on \nthis theory we can tell if the model is an adequate model of the data\nand answer the questions put forward.\n\n## The linear model assumptions\n\nThis section briefly introduces the linear model and the \ntypical assumptions made. This settles notation for the case study \nin the following section. In a first reading it can be read quickly and\nreturned to later to better digest the model assumptions and\ntheir implications.  \n\n[The entries of $X$ go by many other names: explanatory \nvariables, covariates, independent variables, regressors, inputs or features. \nSimilarly, $Y$ is also called the outcome or dependent variable.]{.aside}\nThe linear model relates a continuous *response*\nvariable $Y$ to a $p$-dimensional vector $X$ of *predictors* via the identity\n$$ \n    E(Y \\mid X) = X^T \\beta.\n$$ {#eq-linreg}\nHere \n$$\n    X^T \\beta = X_1 \\beta_1 + \\ldots +  X_p \\beta_p\n$$\nis a linear combination of the predictors weighted by the\n$\\beta$-parameters and $E(Y \\mid X)$ denotes the conditional \nexpectation of $Y$ given $X$. Thus the \nlinear\\sidenote[][-1cm]{The linearity that matters for statistics is the linearity \n in the unknown parameter vector $\\beta$.} model is a model of the conditional\nexpectation of the response variable given the predictors.\n\nAn *intercept* parameter, $\\beta_0$, is often added,\n$$\n    E(Y \\mid X) = \\beta_0 + X^T \\beta.\n$$\nIt is notationally convenient to assume that the intercept \nparameter is included among the other parameters. This can\nbe achieved by joining the predictor $X_0 = 1$ to $X$, thereby\nincreasing the dimension to $p + 1$. In the general presentation we will not \npay particular attention to the intercept. We will assume that if an\nintercept is needed, it is appropriately included among the other parameters, \nand we will index the predictors from $1$ to $p$. Other choices of index set, e.g., from $0$ to $p$, may\nbe convenient in specific cases. \n\nIn addition to the fundamental linearity expressed by @eq-linreg we will \nfor various reasons need additional model assumptions. For later reference we collect \nthree standard model assumptions here.\n\n\\index{assumptions!A1--A3}\n\n**A1.**\n    The conditional expectation of $Y$ given $X$ is\n    $$E(Y \\mid X) = X^T \\beta.$$\n[Assumption A2 is known as *homoskedasticity*, which is derived from the Greek \nwords \"homo\" (same) and \"skedastios\" (dispersion). The opposite is\n*heteroskedasticity*.]{.aside}\n\n**A2.**\n    The conditional variance of $Y$ given $X$ does not \n    depend upon $X$,\n    $$V(Y \\mid X) = \\sigma^2.$$\n\n**A3.**\n    The conditional distribution of $Y$ given $X$ is a normal\n    distribution, \\index{distribution!normal}\n    $$Y \\mid X \\sim \\mathcal{N}(X^T\\beta, \\sigma^2).$$\n\nIt is, of course, implicit in A1 and A2 that $Y$ has finite expectation and variance, respectively.\nIt may not be obvious why Assumption A2 is needed, but it is at least conceivable that A2 makes it easier to estimate the\nvariance, since it doesn't depend upon $X$. The assumption has, furthermore, several \nconsequences for the more technical aspect of the statistical analysis as well as the\ninterpretation of the resulting model and the assessment of the \naccuracy of model predictions. \n\nAssumption A3 is a strong distributional assumption, which is unnecessarily\nrestrictive for many purposes. Often A2 is sufficient, but whether A3 holds \ncan be important when using the model to make probabilistic predictions, e.g., \nwhen computing prediction intervals. It is, in addition, only under \nAssumption A3 that an exact statistical theory can be developed for finite \nsample sizes. Some results used in practice are formally derived under this \nassumption, and they must be regarded as approximations when A3 is violated.\n\nThere exists a bewildering amount of terminology related to the linear\nmodel in the literature. Notation and terminology has been developed\ndifferently for different submodels of the linear model. \nIf the $X$-vector only represents continuous\nvariables, the model is often referred to as the linear *regression*\nmodel. Since any categorical variable can be encoded via $k$ binary dummy variables \n(also known as a one-hot encoding),\nthe linear model includes\nall ANalysis Of VAriance (ANOVA) models. \n[**Dummy variable encoding:** A categorical variable on $k$ levels is encoded as a $k$-dimensional \nbinary vector, whose $j$-th dummy variable equals $1$ if \nthe value of the categorical variable equals the $j$-th level and $0$ otherwise.]{.aside}\nCombinations, known \nin parts of the literature as ANalysis of COVAriance (ANCOVA), are of \ncourse also possible. \n\nTreating each special case separately and using special terminology\nis unnecessary and confusing -- and most likely a consequence of \nhistorically different needs in different areas of\napplications. We give a unified treatment of the linear model.\nIt is a fairly simple model with a rather complete theoretical basis. That said, many \nmodeling questions still have to be settled for any specific real data\nanalysis, which makes applications of even the simple linear model\nnon-trivial. \n\nFor statistical applications we have a sample of $n$ observations and \nto develop the statistical theory we need additional distributional assumptions \nregarding their joint distribution. Letting  $Y_1, \\ldots, Y_n$ \ndenote the $n$ observations of the response with corresponding\npredictors $X_1, \\ldots, X_n$, we collect the responses into a column vector $\\mathbf{Y}$, and \nwe collect the predictors into an $n \\times p$ matrix $\\mathbf{X}$ called the *model matrix*. \nThe $i$-th row of $\\mathbf{X}$ is $X_i^T$. The additional assumptions are:\n\n\\index{assumptions!A4 and A5}\n\n**A4:**\nThe conditional distribution of $Y_i$ given $\\mathbf{X}$ depends upon $X_i$ only,\nand $Y_i$ and $Y_j$ are conditionally *uncorrelated* given $\\mathbf{X}$,\n$$\\text{cov}(Y_i, Y_j \\mid \\mathbf{X}) = 0.$$\n\n**A5:**\nThe conditional distribution of $Y_i$ given $\\mathbf{X}$ depends upon $X_i$ only,\nand $Y_1, \\ldots, Y_n$ are conditionally *independent* given $\\mathbf{X}.$\n\n**A6:**\nThe pairs $(X_1, Y_1), \\ldots, (X_n, Y_n)$ are i.i.d.\n\nThere is a hierarchy among these three assumptions with A6 implying A5 and \nA5 implying A4. Thus A4 is the weakest distributional assumption about the \njoint distribution of the observations.\n\nAssumption A4 implies together with A1 and A2 that \n$$\n    E(\\mathbf{Y} \\mid \\mathbf{X}) = \\mathbf{X} \\beta,\n$$ {#eq-condmean}\nand that \n$$\nV(\\mathbf{Y} \\mid \\mathbf{X}) = \\sigma^2 \\mathbf{I}\n$$ {#eq-condvar}\nwhere $\\mathbf{I}$ denotes the $n \\times n$ identity matrix. \n\n\\begin{marginfigure}[-1cm]\n\\begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}] \n\\matrix (m) [matrix of math nodes, row sep=2em, column sep=0.5em, text height=1.5ex, text depth=0.25ex] {\n &  & \\mathbf{X} &  &  \\\\ \nX_1 & X_2 & \\ldots & X_{p-1} & X_p \\\\ \nY_1 & Y_2 & \\ldots & Y_{p-1} & Y_p \\\\}; \n\\path[-] \n(m-1-3) edge (m-2-1)\n(m-1-3) edge (m-2-2)\n(m-1-3) edge (m-2-4)\n(m-1-3) edge (m-2-4)\n(m-1-3) edge (m-2-5)\n(m-2-1) edge (m-3-1)\n(m-2-2) edge (m-3-2)\n(m-2-4) edge (m-3-4)\n(m-2-5) edge (m-3-5);\n\\end{tikzpicture}\n\\caption{Graphical illustration of the assumptions on the joint distribution. The $Y_i$ depends on $\\mathbf{X}$ through $X_i$ only, and the $Y_i$-s are independent given $\\mathbf{X}$. }\n\\end{marginfigure}\n\nAssumptions A5 and A3 imply that \n$$\n    \\mathbf{Y} \\mid \\mathbf{X} \\sim \\mathcal{N}(\\mathbf{X}\\beta, \\sigma^2 \\mathbf{I}).\n$$ {#eq-conddis}\nIn summary, there are essentially two sets of distributional assumptions. The weak set A1, A2 and\nA4, which imply the moment identities ([-@eq-condmean]) and\n([-@eq-condvar]), and the strong set A3 and A5, which, in addition, imply the\ndistributional identity ([-@eq-conddis]). Occasionally, A6 combined with A1 and A2\nis used to justify certain resampling based procedures, such as bootstrapping,\nor to give asymptotic justifications of inference methodologies. \n\nThe linear regression model and the different assumptions related to it can also \nbe phrased in terms of an additive error. The equation \n$$Y_i = X_i^T \\beta + \\epsilon_i.$$\ndefines $\\epsilon_i$  -- known as the \\marginnote[-1cm]{The additive error.}*error* or \n*noise* term. In terms of $\\epsilon_i$ the model assumptions are as follows: \n\n*   A1 is equivalent to $E(\\epsilon_i \\mid X_i) = 0$\n*   A2 is equivalent to $V(\\epsilon_i \\mid X) = \\sigma^2$\n*   A3 is equivalent to $\\epsilon_i \\mid X_i \\sim \\mathcal{N}(0, \\sigma^2)$. \n\nNote that under Assumption A3, the conditional distribution of $\\epsilon_i$ given \n$X_i$ does not depend upon $X_i$, thus A3 implies that $\\epsilon_i$ and $X_i$ are independent. \n\nIt is important to realize that the model assumptions cannot\nbe justified prior to the data analysis. There are no magic arguments or\nsimple statistical summaries that imply that the assumptions are fulfilled. \nA histogram of the marginal distribution of the responses can, \nfor instance, not be used as an argument for or against Assumption A3 on the \nconditional normal distribution of $Y$. Justifications and investigations of \nmodel assumptions are done *after* a model has been fitted to data. This is called \n*model diagnostics*. \n\n## Linear models of claim sizes\n\nReturning to the insurance claim sizes, a regression model of\nlog claim size, $\\log(Y_i)$, for insured $i$ using log insurance sum,\n$\\log(X_{i, \\texttt{sum}})$, as predictor can be fitted to data using the\nR function `lm()`. The theory behind the computations is treated in\nthe subsequent section.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nclaims_lm <- lm(\n    log(claims) ~ log(sum), \n    data = claims\n)\ncoefficients(claims_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    log(sum) \n  5.8410060   0.2114886 \n```\n\n\n:::\n:::\n\n\nThis model is the linear model\n$$E(\\log(Y_i) \\mid X_{i, \\texttt{sum}}) = \\beta_0 + \\beta_{\\texttt{sum}} \\log(X_{i, \\texttt{sum}}).$$\nIn the R terminology the parameter $\\beta_0$ is the `(Intercept)` coefficient\nand $\\beta_{\\texttt{sum}}$ is the `log(sum)` coefficient. The model\ncorresponds to the regression line shown in @fig-claims-scatter.\n\nNote that in concrete cases we will not write the conditioning variable explicitly as\n$\\log(X_{i, \\texttt{sum}})$. As long as the right hand side is a linear combination\nof transformations of the conditioning variables (and an intercept) this is a linear model.\nIt would be bothersome to write out explicitly the vector of transformations on the\nleft hand side. These are, however, computed explicitly by \\texttt{lm} and stored in the\nmodel matrix $\\mathbf{X}$.\n\n\\marginnote[1cm]{The rows in the model matrix shown were chosen to illustrate certain\ndifferences in subsequent examples.}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nmodel.matrix(claims_lm)[781:784, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept) log(sum)\n781           1 19.42274\n782           1 14.63276\n783           1 14.90718\n784           1 15.24711\n```\n\n\n:::\n:::\n\n\nIn the general treatment of the linear model it is notationally beneficial to\nalways condition explicitly on the model matrix or a row in the model matrix. Thus to\nhave a smooth notation in the general theory as well as in the concrete examples\nwe will have to live with this slight discrepancy  in notation. This will usually not be a\nsource of confusion once the nature of this discrepancy is understood clearly. The models\ntreated below illustrate this point further.\n\nWe can condition on the trade group, $\\texttt{grp}_i$, as well and thus consider\nthe model\n\\marginnote{The dummy variable encoding is $$X_{i,\\texttt{grp}k} = 1(\\texttt{grp}_i = k).$$}\n$$\n\\begin{aligned}\nE(\\log(Y_i) \\mid X_{i, \\texttt{sum}}, \\texttt{grp}_i) & = \\beta_0 + \\beta_{\\texttt{sum}} \\log(X_{i, \\texttt{sum}}) \\\\\n& \\hskip -1cm  + \\beta_{\\texttt{grp2}} X_{i, \\texttt{grp2}} +\n\\beta_{\\texttt{grp3}} X_{i, \\texttt{grp3}}  + \\beta_{\\texttt{grp4}} X_{i, \\texttt{grp4}}.\n\\end{aligned}\n$$\nThis model is\nknown as the additive model. The dummy variable encoding of the categorical\ntrade group is used, and since the\nmodel includes an intercept the dummy variable corresponding to the first group is\nremoved.\nOtherwise the model would be overparametrized. The consequence is that the first group\nacts as a baseline, and that the parameters $\\beta_{\\texttt{grp2}}$, $\\beta_{\\texttt{grp3}}$ \nand $\\beta_{\\texttt{grp4}}$ are\nthe differences from the baseline. These differences are also known\nas *contrasts*. \\index{contrasts} We see, for example, that\n$$\n    E(\\log(Y_i) \\mid X_{i, \\texttt{sum}} = x, \\texttt{grp}_i = 1) = \\beta_0 +\n    \\beta_{\\texttt{sum}} \\log(x)\n$$\nand\n$$\n    E(\\log(Y_i) \\mid  X_{i, \\texttt{sum}} = x, \\texttt{grp} = 2) = \\beta_0 +\n    \\beta_{\\texttt{sum}} \\log(x) + \\beta_{\\texttt{grp2}},\n$$\nthus the difference from the baseline trade group 1 to trade group 2 is\n$\\beta_{\\texttt{grp2}}$.\n\nWe can again fit this model using `lm()` in R, which automatically\ndetermines the model parametrization (using defaults) and computes the\nmodel matrix.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nclaims_lm_add <- lm(\n    log(claims) ~ log(sum) + grp,\n    data = claims\n)\ncoefficients(claims_lm_add)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    log(sum)        grp2        grp3        grp4 \n  3.5974043   0.3300459   0.5472730   0.4013465   0.9143058 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nmodel.matrix(claims_lm_add)[781:784, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept) log(sum) grp2 grp3 grp4\n781           1 19.42274    0    0    0\n782           1 14.63276    1    0    0\n783           1 14.90718    0    0    1\n784           1 15.24711    1    0    0\n```\n\n\n:::\n:::\n\n\nAlternative parametrizations can be specified using the `contrasts`\nargument.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nclaims_lm_add <- lm(\n    log(claims) ~ log(sum) + grp, \n    data = claims,\n    contrasts = list(grp = \"contr.sum\")\n)\ncoefficients(claims_lm_add)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    log(sum)        grp1        grp2        grp3 \n 4.06313567  0.33004591 -0.46573135  0.08154169 -0.06438483 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nmodel.matrix(claims_lm_add)[781:784, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept) log(sum) grp1 grp2 grp3\n781           1 19.42274    1    0    0\n782           1 14.63276    0    1    0\n783           1 14.90718   -1   -1   -1\n784           1 15.24711    0    1    0\n```\n\n\n:::\n:::\n\n\nThe `contr.sum` contrast gives the parametrization\n$$\n\\begin{aligned}\nE(\\log(Y_i) \\mid X_{i, \\texttt{sum}}, \\texttt{grp}_i) & = \\beta_0 + \\beta_{\\texttt{sum}} \\log(X_{i, \\texttt{sum}}) \\\\\n& + \\beta_{\\texttt{grp1}} X_{i, \\texttt{grp1}} + \\beta_{\\texttt{grp2}} X_{i, \\texttt{grp2}} \\\\\n& + \\beta_{\\texttt{grp3}} X_{i, \\texttt{grp3}}  + \\beta_{\\texttt{grp4}} X_{i, \\texttt{grp4}}.\n\\end{aligned}\n$$\nwith the constraint $\\beta_{\\texttt{grp1}} + \\beta_{\\texttt{grp2}} + \\beta_{\\texttt{grp3}} + \\beta_{\\texttt{grp4}} = 0$. Only the parameters $\\beta_{\\texttt{grp1}}$, $\\beta_{\\texttt{grp2}}$ and $\\beta_{\\texttt{grp3}}$ are reported, and $\\beta_{\\texttt{grp4}} = - \\beta_{\\texttt{grp1}} - \\beta_{\\texttt{grp2}} - \\beta_{\\texttt{grp3}}$ as can also be seen from the model matrix. This latter parametrization can be\nmore convenient if we want to compare trade groups to the overall intercept instead of\nto one specific baseline group. \n\nThe additive model gives a different intercept for each trade group but the\nslope is kept the same for all trade groups. Within the framework of linear\nmodels we can capture differences in slope as an *interaction* \\index{interaction}\nbetween the insurance sum and the trade group. Using the parametrization with trade group\n1 as baseline this model is\n$$\n\\begin{aligned}\nE(\\log(Y_i) \\mid X_{i, \\texttt{sum}}, \\texttt{grp}_i) & = \\beta_0 + \\beta_{\\texttt{sum}} \\log(X_{i, \\texttt{sum}}) \\\\\n& + (\\beta_{\\texttt{grp2}} + \\beta_{\\texttt{sum}, \\texttt{grp2}} \\log(X_{i, \\texttt{sum}}))X_{i, \\texttt{grp2}} \\\\\n& + (\\beta_{\\texttt{grp3}} + \\beta_{\\texttt{sum}, \\texttt{grp3}} \\log(X_{i, \\texttt{sum}}))X_{i, \\texttt{grp3}} \\\\\n& + (\\beta_{\\texttt{grp4}} + \\beta_{\\texttt{sum}, \\texttt{grp4}} \\log(X_{i, \\texttt{sum}}))X_{i, \\texttt{grp4}}.\n\\end{aligned}\n$$\n\nThe parameter estimates and model matrix are:\n\n\n::: {.cell .column-page-right layout-align=\"center\"}\n\n```{.r .cell-code}\ncoefficients(claims_lm_int) |> print(width = 120)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)      log(sum)          grp2          grp3          grp4 log(sum):grp2 log(sum):grp3 log(sum):grp4 \n  3.525657166   0.334288387   0.434262343   3.666489986   0.040781595   0.007719206  -0.209986484   0.059341891 \n```\n\n\n:::\n\n```{.r .cell-code}\nmodel.matrix(claims_lm_int)[781:784, ] |> print(width = 120)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept) log(sum) grp2 grp3 grp4 log(sum):grp2 log(sum):grp3 log(sum):grp4\n781           1 19.42274    0    0    0       0.00000             0       0.00000\n782           1 14.63276    1    0    0      14.63276             0       0.00000\n783           1 14.90718    0    0    1       0.00000             0      14.90718\n784           1 15.24711    1    0    0      15.24711             0       0.00000\n```\n\n\n:::\n:::\n\n\nThis model corresponds to the regression lines shown in\n@fig-claims-scatter-strat. This example illustrates why we do not want to\nwrite out the eight dimensional vector explicity in the condition when in fact\nit is just a transformation of $X_{i, \\texttt{sum}}$ and $\\texttt{grp}_i$. On the other\nhand, it would be equally bothersome to drag an abstract transformation around\nin the general notation.\n\nThree linear models were considered in this section, and subsequent sections\nand chapters will treat different ways of comparing models and for drawing\ninference about model parameters and hypotheses. It must be emphasized that\nthe purpose of developing the theory and the corresponding statistical\nmethods is **not** to be able to deduce from data what the \"true model\"\nis. There are several reasons for that. For once, all models should be \nconsidered as approximations only.\n[*All models are wrong but some are useful.* George Box, 1978]{.aside}\nWe can try to find an adequate model approximation\nthat captures essential aspects of the data, and we may be able to justify\nthat a model is adequate, but we will never be able to justify that a model\nis \"true\". \n\nSeveral models may also be valid and yet\nappear contradictory, and it may then depend on the purpose of the modeling\nwhich to use. It is, for instance, important to formulate precisely which\npredictors that we condition on. Suppose that the conditional expectation\n$E(\\log(Y_i) \\mid X_{i, \\texttt{sum}}, \\texttt{grp}_i)$ is given by the additive\nmodel, say, then it follows from the tower property of conditional expectations that\n$$\n\\begin{aligned}\nE(\\log(Y_i) \\mid X_{i, \\texttt{sum}}) & = \\beta_0\n+ \\beta_{\\texttt{sum}} \\log(X_{i, \\texttt{sum}}) \\\\\n& + \\beta_{\\texttt{grp2}} P(\\texttt{grp}_i = 2 \\mid X_{i, \\texttt{sum}})  \\\\\n& + \\beta_{\\texttt{grp3}} P(\\texttt{grp}_i = 3 \\mid X_{i, \\texttt{sum}}) \\\\\n& + \\beta_{\\texttt{grp4}} P(\\texttt{grp}_i = 4 \\mid X_{i, \\texttt{sum}}).\n\\end{aligned}\n$$\n\nNow if [The approximation can be justified locally using Taylor's theorem\nif the probability depends smoothly on the predictor.]{.aside}\n$$ \n    P(\\texttt{grp}_i = k \\mid X_{i, \\texttt{sum}}) \\approx \\delta_k + \\gamma_k \\log(X_{i, \\texttt{sum}})\n$$\nfor $k = 2,3, 4$ then the linear model\n$$\n    E(\\log(Y_i) \\mid X_{i, \\texttt{sum}}) = \\beta_0\n+ \\beta_{\\texttt{sum}} \\log(X_{i, \\texttt{sum}})\n$$\nwill be an approximately valid model as well. However, the intercept and slope for this\nmodel may differ from the model where we condition also on the group. This depends\non whether or not the $\\beta_{\\texttt{grp}k}$, $\\delta_k$ and\n$\\gamma_k$ parameters are 0.  If the $\\beta_{\\texttt{grp}k}$ parameters were all 0 it would not matter if group is included in the conditioning. If group and insurance sum were independent then\n$\\gamma_k = 0$, and both models would be equally valid models of the conditional\nexpectation of the log claim size. The slope would then be the same in both\nmodels but the intercept would differ. If group and insurance sum were dependent then the slope would depend on\nwhether we condition on insurance sum only or whether we condition on insurance sum\nas well as group. \n\nThe key take-home message is that a predictor variable may be excluded\nfrom the model even if it has a non-zero coefficient without necessarily invalidating\nthe model. Another take-home message is that there is no such thing as an unqualified \"effect\"\nof a predictor variable. Effect sizes, as quantified by the $\\beta$ coefficients, are\nalways depending upon the precise choice of conditioning variables, whether this is\nmade explicit by the modeling or is implicit by the data sampling process.\n\nIn the light of George Box's famous aphorism above it is only appropriate to give another\nBox quote: *Since all models are wrong the scientist must be alert to what is\nimportantly wrong* [@Box:1976]. One place to start is to investigate the assumptions\nA1, A2 and A3. These are investigated after the model has been fitted to data\ntypically via the *residuals*\n$$\n    \\hat{\\epsilon}_i = Y_i - X_i^T \\hat{\\beta}\n$$ {#eq-resid-def} \\index{residual!linear model}\nthat are approximations of the errors $\\epsilon_i$. \\index{residual!plot}\n\n\n::: {.cell .caption-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nclaims_diag <- augment(claims_lm) ## Residuals etc.\ngridExtra::grid.arrange(\n  ggplot(claims_diag, aes(.fitted, .resid), alpha = I(0.2)) + geom_point() + geom_smooth(),\n  ggplot(claims_diag, aes(.resid)) + geom_histogram(bins = 40),\n  ggplot(claims_diag, aes(sample = .std.resid)) + geom_qq() + geom_abline(),\n  ncol = 3\n)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots based on residuals for the regression model conditioning on insurance sum only. The residual plot (left) and the histogram (middle) use the raw residuals. The qq-plot (right) uses standardized residuals.](2_linear_files/figure-html/fig-claims-diag-1.png){#fig-claims-diag fig-align='center' width=864}\n:::\n:::\n\n\n@fig-claims-diag shows three typical diagnostic plots. The residuals plotted\nagainst the fitted values, $X_i^T \\hat{\\beta}$, can be used to detect if the mean value\nspecification is adequate and if the variance is approximately constant as a function of\nthe fitted values. The scatter plot smoother added to the residual plot bends up in the\nends indicating that the mean is not spot on. There is no clear indication of variance\ninhomogeneity. The histogram of the residuals and the qq-plot of the standardized\\sidenote[]{Standardized\nso as to make the residuals have unit variance under the model assumptions.} residuals\ncan be used for comparing the residual distribution with the normal distribution. In\nthis case the residuals have a slightly right skewed distribution when compared\nto the normal distribution. Of course we also note the pattern of the most extreme\npositive residuals that correspond to the capped claims. In this section we will not\npursue the effect of those.\n\n\n\n::: {.cell .caption-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nclaims_diag_int <- augment(claims_lm_int) ## Residuals etc.\ngridExtra::grid.arrange(\n  ggplot(claims_diag_int, aes(.fitted, .resid), alpha = I(0.2)) + geom_point() + geom_smooth(),\n  ggplot(claims_diag_int, aes(.resid)) + geom_histogram(bins = 40),\n  ggplot(claims_diag_int, aes(sample = .std.resid)) + geom_qq() + geom_abline(),\n  ncol = 3\n)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots based on residuals for the regression model with an interaction of trade group and insurance sum. The residual plot (left) and the histogram (middle) use the raw residuals. The qq-plot (right) uses standardized residuals.](2_linear_files/figure-html/fig-claims-diag-int-1.png){#fig-claims-diag-int fig-align='center' width=864}\n:::\n:::\n\n\n@fig-claims-diag-int shows diagnostic plots for the more complicated interaction\nmodel. The conclusion is the same as for the simpler model. The mean value model is\nnot spot on and the residual distribution is slightly skewed. This is not a bad\nmodel fit but it leaves room for improvements.\n\n## Estimation theory {#sec-estimation-theory}\n\nThe theory that we will cover in this section is on the estimation of the \nunknown parameter $\\beta$ using least squares methods. We give theoretical results on the existence \nand uniqueness, and we provide characterizations of the least squares estimator. We \nalso discuss how the estimator is computed in practice. \n\n### Weighted linear least squares estimation\n\\index{weighted least squares}\n\nWe will consider the generalization of linear least squares that \namong other things allows for weights on the individual cases. Allowing for weights\ncan be of interest in itself, but serves, in particular, as a preparation for the \nmethods we will consider in @chap-glm.\n\nWe introduce the *weighted squared error loss*\n[With $\\mathbf{W} = \\mathbf{I}$ this loss is proportional to \nthe negative log-likelihood loss under assumptions A3 and A5 as derived in @chap-glm.]{.aside}\n\\index{loss!squared error} \\index{weighted squared loss} as \n$$ \n\\ell(\\beta) = (\\mathbf{Y} - \\mathbf{X}\\beta)^T\\mathbf{W}(\\mathbf{Y}\n- \\mathbf{X}\\beta)\n$$ {#eq-wsel}\nwhere $\\mathbf{W}$ is a positive definite matrix. An $n \\times n$ matrix is positive definite if \nit is symmetric and\n$$\\mathbf{y}^T \\mathbf{W} \\mathbf{y} > 0$$ \nfor all $\\mathbf{y} \\in \\mathbb{R}^n$ with $\\mathbf{y} \\neq 0$. A\nspecial type of positive definite weight matrix is a diagonal matrix with positive\nentries in the diagonal. With  \n$$\n    \\mathbf{W} = \\left(\\begin{array}{cccc} \n    w_1 & 0 & \\ldots & 0 \\\\\n    0 & w_2 & \\ldots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\ldots & w_n \\end{array}\\right)\n$$\nwe find that the weighted squared error loss becomes\n$$\n    \\ell(\\beta) =  \\sum_{i} w_i (Y_i - X_i^T \\beta)^2.\n$$\nThat is, the $i$-th case receives the weight $w_i$. \n\\index{weight matrix} \\index{penalty matrix}\n\nWe will estimate the parameter $\\beta$ via minimization of $\\ell$ -- or via \nminimization of $\\ell$ plus a penalty term. With $\\boldsymbol{\\Omega}$ \na positive semidefinite $p \\times p$ matrix we will seek to minimize the function  \n$$ \n\\ell_{\\boldsymbol{\\Omega}}(\\beta) := \\ell(\\beta) + \\beta^T \\boldsymbol{\\Omega} \\beta.\n$$ {#eq-penll}\nThe penalty term, $\\beta^T \\boldsymbol{\\Omega} \\beta$, introduces a form of control on the \nminimization problem, and thus the estimator, by shrinking the minimizer toward 0. This can have \nbeneficial numerical as well as statistical consequences. The purpose of introducing a penalty will be elaborated upon in subsequent sections, where some concrete examples will be given.   \n\n\\index{normal equation}\n[@eq-normal is called the *normal* equation.]{.aside}\n\n::: {#thm-normal} \nA solution to the equation \\index{normal equation}\n$$\n\\left(\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\boldsymbol{\\Omega}\\right) \\beta = \n\\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$ {#eq-normal}\nis a minimizer of the quadratic loss function ([-@eq-penll]). There is a unique minimizer if either \n$\\mathbf{X}$ has full column rank $p$ or if $\\boldsymbol{\\Omega}$ is positive definite. \n::: \n\n::: {.proof} \nThe derivative of $\\ell_{\\boldsymbol{\\Omega}}$ is \n$$\n    D_{\\beta} \\ell_{\\boldsymbol{\\Omega}}(\\beta) = -2(\\mathbf{Y} - \\mathbf{X}\\beta)^T \\mathbf{W} \\mathbf{X} + 2 \\beta^T \\boldsymbol{\\Omega}.\n$$\n\nTo compute this derivative, it may be useful to think of $\\ell$ as a\ncomposition. The function \n$a(\\beta) = (\\mathbf{Y} - \\mathbf{X}\\beta)$ from $\\mathbb{R}^{p}$ to $\\mathbb{R}^n$ has\nderivative $D_{\\beta}a(\\beta) = -\\mathbf{X}$, and $\\ell$ is a composition of $a$\nwith the function $b(z) = z^T \\mathbf{W} z$ from $\\mathbb{R}^n$ to $\\mathbb{R}$ with derivative \n$D_z b(z) = 2 z^T \\mathbf{W}$. By the chain rule\n$$ \n    D_{\\beta} \\ell(\\beta) = D_zb(a(\\beta)))D_{\\beta}a(\\beta) = -2(\\mathbf{Y} - \\mathbf{X}\\beta)^T \\mathbf{W} \\mathbf{X}.\n$$\n\nThe derivative of $\\beta \\mapsto \\beta^T \\boldsymbol{\\Omega} \\beta$ is found in the same way \nas the derivative of $b$. Note that the derivative is a *row vector*.[^column-gradient] \n\nWe note that @eq-normal is equivalent to \n$D_{\\beta} \\ell_{\\boldsymbol{\\Omega}}(\\beta) = 0$, \nhence a solution of @eq-normal is a stationary point of $\\ell_{\\boldsymbol{\\Omega}}$.\n\nThe second derivative of $\\ell_{\\boldsymbol{\\Omega}}$ is \n$$\n    D_{\\beta}^2 \\ell_{\\boldsymbol{\\Omega}}(\\beta) = \n    2 \\left(\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\boldsymbol{\\Omega}\\right).\n$$\nThis matrix is (globally) positive semidefinite, hence the function is convex and \nany stationary point is a global minimum. \n\nIf $\\mathbf{X}$ has rank $p$ or if $\\boldsymbol{\\Omega}$ is positive definite, \n$D_{\\beta}^2 \\ell_{\\boldsymbol{\\Omega}}(\\beta)$ is positive definite and the minimizer of \n$\\ell_{\\boldsymbol{\\Omega}}$ is the unique solution of @eq-normal.\n:::\n\n[^column-gradient]: The *gradient*, $$\\nabla_{\\beta}  \\ell(\\beta) = D_{\\beta} \\ell(\\beta)^T,$$ is a column vector.\n\n\nWhen the solution to @eq-normal is unique it can, of course, be written as \n$$\n    \\hat{\\beta} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + {\\boldsymbol{\\Omega}})^{-1}\n    \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}.\n$$ \nThis can be a theoretically convenient formula for the estimator, but as \nwe discuss below, the practical computation of $\\hat{\\beta}$ does typically \nnot rely on explicit matrix inversion. \n\n[In the rest of this section we only consider the case with ${\\boldsymbol{\\Omega}} = 0$.]{.aside}\nA geometric interpretation of the weighted least squares estimator \nwithout penalization provides additional insights. The inner product induced by $\\mathbf{W}$\non $\\mathbb{R}^n$ is given by $\\mathbf{y}^T \\mathbf{W} \\mathbf{x}$, and the \ncorresponding norm is denoted $\\|\\cdot\\|_{\\mathbf{W}}$. With this notation \nwe see that \n$$\n    \\ell(\\beta) = \\|\\mathbf{Y} - \\mathbf{X}\\beta\\|^2_{\\mathbf{W}}.\n$$ \nIf $L = \\{\\mathbf{X}\\beta  \\mid \\beta \\in \\mathbb{R}^p\\}$ denotes the column\nspace of $\\mathbf{X}$, $\\ell$ is minimized whenever $\\mathbf{X}\\beta$ is the \northogonal projection of $\\mathbf{Y}$ onto $L$ in the inner product given by\n$\\mathbf{W}$.\n[$\\|\\mathbf{y}\\|_{\\mathbf{W}}^2 = \\mathbf{y}^T \\mathbf{W} \\mathbf{y}$ specifies a norm if and only if $\\mathbf{W}$ is positive definite.]{.aside}\n\n::: {#lem-proj}\nThe orthogonal projection onto $L$ is \n$$\n    P = \\mathbf{X} (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \n$$\nprovided that $\\mathbf{X}$ has full column rank $p$. \n:::\n\n::: {.proof}\nWe verify that $P$ is the orthogonal projection onto $L$ by verifying three\ncharacterizing properties: \n$$\n    \\begin{align*}\n        P \\mathbf{X} \\beta & = \\mathbf{X} \\beta \\quad (P \\text{ is the identity on } L) \\\\\n        P^2 & = \\mathbf{X} (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \n        \\mathbf{X}^T \\mathbf{W}  \\mathbf{X} \n        (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T\n        \\mathbf{W} \\\\\n        & = \\mathbf{X}\n        (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} = P \\\\\n        P^T \\mathbf{W}  \n        & = (\\mathbf{X} (\\mathbf{X}^T \\mathbf{W}\n        \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} )^T \\mathbf{W} \\\\\n        & =\n        \\mathbf{W} \\mathbf{X} (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T\n        \\mathbf{W} = \\mathbf{W} P.\n    \\end{align*}\n$$\n\nThe last property is self-adjointness w.r.t. the inner product given\nby $\\mathbf{W}$.\n:::\n\n\nNote that for $\\boldsymbol{\\Omega} = 0$, @thm-normal is a consequence of @lem-proj.\nThis follows from the identity $P\\mathbf{Y} = \\mathbf{X} \\hat{\\beta}$, \nsince the equation $P\\mathbf{Y} = \\mathbf{X} \\beta$ has a unique solution\nwhen the columns of $\\mathbf{X}$ are linearly independent.\n\nIf $\\mathbf{X}$ does not have rank $p$ the projection is still\nwell defined, and it can be written as \n$$\n    P = \\mathbf{X} (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-} \\mathbf{X}^T \\mathbf{W}\n$$\nwhere $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-}$ denotes a generalized inverse.\n[A generalized inverse of a matrix $A$ is any matrix $A^{-}$ with the property that $$AA^{-}A = A.$$]{.aside} \nThis is seen by verifying the same three conditions as in the proof\nabove. The solution to $P\\mathbf{Y} = \\mathbf{X} \\beta$ is, however, no longer unique, and the \nsolution \n$$\n    \\hat{\\beta} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\nis just one possible solution.\n\n%% Moore-Penrose pseudoinverse and norm-minimal solutions. \n\n### Algorithms\n\nThe actual computation of the solution to the normal equation is \ntypically based on a QR decomposition of $\\mathbf{X}$ \nand not on a matrix inversion or a direct solution of the linear equation.\n[The normal equation can be solved by Gaussian elimination, \\index{Gaussian elimination} which corresponds to an LU decomposition of $$\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\boldsymbol{\\Omega}.$$\nThe algorithm was named after Carl Friedrich Gauss, though he did not invent it. However, he promoted it \nfor efficient computation of the solution of least squares problems.]{.aside}\nThe R function `lm()` -- \nor rather the underlying R functions \n`lm.fit()` and `lm.wfit()` -- are based on the QR decomposition. We first \ntreat the case $\\mathbf{W} = \\mathbf{I}$ and  $\\boldsymbol{\\Omega} = 0$ and then subsequently the \nmore general cases. \n\n\\index{QR decomposition}\nThe QR decomposition of $\\mathbf{X}$ is the factorization \n$$\n    \\mathbf{X} = \\mathbf{Q} \\mathbf{R},\n$$\nwhere $\\mathbf{Q}$ is an orthogonal matrix and $\\mathbf{R}$ is an upper triangular matrix. \nSince \n$$\n    \\mathbf{X}^T \\mathbf{X}  = \\mathbf{R}^T \\underbrace{\\mathbf{Q}^T \\mathbf{Q}}_{\\mathbf{I}} \\mathbf{R} = \\mathbf{R}^T \\mathbf{R},\n$$ {#eq-cholviaqr}\nthe normal equation becomes\n$$\n    \\mathbf{R}^T \\mathbf{R} \\beta = \\mathbf{X}^T \\mathbf{Y}.\n$$\nThis equation can be solved efficiently and in a numerically stable way in a two-step pass \nby exploiting first that $\\mathbf{R}^T$ is lower triangular and then that $\\mathbf{R}$\nis upper triangular. Moreover, the QR decomposition can be implemented in a rank revealing \nway. This is useful when $\\mathbf{X}$ does not have rank $p$, in which case the \nQR decomposition effectively reparametrizes the model as if some columns were removed from \n$\\mathbf{X}$. Such a version of the QR decomposition is used by \n`lm.fit()` and `lm.wfit()` in R, and one will obtain parameter estimates \nwith the value `NA` when $\\mathbf{X}$ has (numerical) rank strictly less than $p$.\n\\index{Cholesky decomposition}\n\nFor a general weight matrix we write\n[This could be the Cholesky decomposition with $\\mathbf{L}$ lower triangular, or $\\mathbf{L}$ could be the symmetric square root of $\\mathbf{W}$. For a diagonal $\\mathbf{W}$, $\\mathbf{L}$ is diagonal and trivial to compute by taking square roots. For unstructured $\\mathbf{W}$ the computation of the Cholesky decomposition scales as $n^3$.]{.aside}\n$\\mathbf{W} = \\mathbf{L} \\mathbf{L}^T$, in which \ncase the normal equation can be rewritten as \n$$ \n    (\\mathbf{L}^T \\mathbf{X})^T \\mathbf{L}^T \\mathbf{X} \\beta = (\\mathbf{L}^T \\mathbf{X})^T \\mathbf{L}^T \\mathbf{Y}\n$$\nThus by replacing $\\mathbf{X}$ with $\\mathbf{L}^T \\mathbf{X}$ and $\\mathbf{Y}$ with \n$\\mathbf{L}^T \\mathbf{Y}$ we can proceed as above and solve the normal equation via a \nQR decomposition of $\\mathbf{L}^T\\mathbf{X}$.\n\nThe computations based on the QR decomposition don't involve the computation \nof the *cross product* \\index{cross product} $\\mathbf{X}^T \\mathbf{X}$ (or more generally, $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$). The factorization ([-@eq-cholviaqr]) of the \npositive semidefinite matrix $\\mathbf{X}^T \\mathbf{X}$ as a lower and upper triangular matrix \nis called the Cholesky decomposition, and as outlined above it can be computed via the \nQR decomposition of $\\mathbf{X}$. An alternative is to \ncompute $\\mathbf{X}^T \\mathbf{X}$ directly and then compute its Cholesky \ndecomposition. The QR decomposition is usually preferred for numerical stability. Computing\n$\\mathbf{X}^T \\mathbf{X}$ is essentially a squaring operation, and precision can \nbe lost. However, if $n$ is large compared to $p$ it will be faster to compute \nthe Cholesky decomposition from the cross product. \n\nIf $\\boldsymbol{\\Omega} \\neq 0$ the normal equation can be solved by computing \nthe Cholesky decomposition \n$$\n    \\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\boldsymbol{\\Omega} = \\mathbf{R}^T \\mathbf{R}\n$$\ndirectly and then proceed as above. \n\nTo use the QR decomposition we write\n[The computations could again be done via the Cholesky decomposition of $\\boldsymbol{\\Omega}$, \nor $\\mathbf{D}$ could be computed as the symmetric square root of $\\boldsymbol{\\Omega}$.]{.aside} \n$\\boldsymbol{\\Omega} = \\mathbf{D} \\mathbf{D}^T$. Then the normal equation can be written as \n$$\n    \\left(\\begin{array}{cc}  \\mathbf{X}^T \\mathbf{L} & \\mathbf{D} \\end{array} \\right) \\left(\\begin{array}{c} \\mathbf{L}^T \\mathbf{X} \\\\ \\mathbf{D}^T \\end{array} \\right) \\beta = \n\\left(\\begin{array}{cc}  \\mathbf{X}^T\\mathbf{L} & \\mathbf{D} \\end{array} \\right)\n\\left(\\begin{array}{c} \\mathbf{L}^T \\mathbf{Y} \\\\ 0 \\end{array} \\right).\n$$\nBy replacing $\\mathbf{X}$ with\n$$\n    \\left(\\begin{array}{c} \\mathbf{L}^T \\mathbf{X} \\\\ \\mathbf{D}^T \\end{array} \\right)\n$$\nand $\\mathbf{Y}$ with\n$$\n    \\left(\\begin{array}{c} \\mathbf{L}^T \\mathbf{Y} \\\\ 0 \\end{array} \\right)\n$$\nwe can solve the normal equation via the QR decomposition as above. \n\n## Sampling distributions {#sec-sampling-distributions}\n\nIn this section we give results on the distribution of the estimator, $\\hat{\\beta}$, \nand test statistics of linear hypotheses under the weak assumptions A1, A2 and A4\nand under the strong assumptions A3 and A5. Under the weak assumptions we \ncan only obtain results on moments, while the strong distributional \nassumptions give exact sampling distributions. Throughout we restrict attention to the \ncase where $\\mathbf{W} = \\mathbf{I}$ and $\\boldsymbol{\\Omega} = 0$.\n\n### Moments\n\nSome results involve the unknown variance parameter $\\sigma^2$\n(see Assumption A2) and some involve a specific estimator $\\hat{\\sigma}^2$. This \nestimator of $\\sigma^2$ is \n\\index{variance estimator!unbiased}\n$$ \n    \\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} (Y_i -\n    X_i^T\\hat{\\beta})^2 =  \\frac{1}{n-p} \\|\\mathbf{Y} -\n    \\mathbf{X}\\hat{\\beta}\\|^2\n$$ {#eq-varest} \nprovided that $\\mathbf{X}$ has full rank $p$. Recalling that the $i$-th residual is\n$$\\hat{\\epsilon}_i = Y_i - X_i^T\\hat{\\beta},$$\nthe variance estimator equals the empirical variance of the residuals -- \nup to division by $n-p$ and not $n$. Since the residual is a \nnatural estimator of the unobserved error $\\epsilon_i$, the\nvariance estimator $\\hat{\\sigma}^2$ is a natural estimator of \nthe error variance $\\sigma^2$. The explanation of the denominator $n-p$ \nis related to the fact that $\\hat{\\epsilon}_i$ is an estimator of $\\epsilon_i$.\nA partial justification, as shown in the following theorem, is that division by $n-p$ \nmakes $\\hat{\\sigma}^2$ unbiased.\n\n::: {#thm-OLS-moments}\nUnder the weak assumptions A1, A2 and A4, and assuming that $\\mathbf{X}$ has full rank $p$,\n$$\n    \\begin{align*}\n    E(\\hat{\\beta} \\mid \\mathbf{X}) & = \\beta, \\\\\n    V(\\hat{\\beta} \\mid \\mathbf{X}) & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}, \\\\\n    E(\\hat{\\sigma}^2 \\mid \\mathbf{X}) &= \\sigma^2.\n    \\end{align*}\n$$\n:::\n\n::: {.proof}\nUsing assumptions A1 and A4 we find that \n$$\n    \\begin{align*}\n    E(\\hat{\\beta} \\mid \\mathbf{X}) & = \n    E((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y} \\mid \\mathbf{X}) \\\\\n    & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E(\\mathbf{Y} \\mid \\mathbf{X}) \\\\\n    & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{X} \\beta \\\\\n    & = \\beta.\n    \\end{align*}\n$$\nUsing assumptions A2 and A4 it follows that \n$$\n    \\begin{align*}\n    V(\\hat{\\beta} \\mid \\mathbf{X}) & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T V(\\mathbf{Y} \\mid \\mathbf{X})  \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n    & =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\sigma^2 \\mathbf{I} \\mathbf{X}\n    (\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n    & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}.\n \\end{align*}\n$$\n\nFor the computation of the expectation of $\\hat{\\sigma}^2$, the geometric \ninterpretation of $\\hat{\\beta}$ is useful. Since $\\mathbf{X} \\hat{\\beta} = P \\mathbf{Y}$\nwith $P$ the orthogonal projection onto the column space $L$ of $\\mathbf{X}$, we find that \n$$\n    \\mathbf{Y} - \\mathbf{X} \\hat{\\beta} = (\\mathbf{I} - P) \\mathbf{Y}.\n$$\nBecause $E(\\mathbf{Y} - \\mathbf{X} \\hat{\\beta} \\mid \\mathbf{X}) = 0$ \n$$\n    E(\\|\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}\\|^2 \\mid \\mathbf{X}) = \\sum_{i=1}^n V(\\mathbf{Y} - \\mathbf{X} \\hat{\\beta} \\mid \\mathbf{X})_{ii}\n$$\nand  \n$$\n    \\begin{align*}\n    V(\\mathbf{Y} - \\mathbf{X} \\hat{\\beta} \\mid \\mathbf{X}) & = V((\\mathbf{I} - P) \\mathbf{Y} \\mid \\mathbf{X}) \\\\\n    & = (\\mathbf{I} - P) V(\\mathbf{Y} \\mid \\mathbf{X}) (\\mathbf{I} - P)^T \\\\\n    & = (\\mathbf{I} - P) \\sigma^2 \\mathbf{I} (\\mathbf{I} - P) \\\\\n    & = \\sigma^2 (\\mathbf{I} - P).\n    \\end{align*}\n$$\nThe sum of the diagonal elements in $(\\mathbf{I} - P)$ is the trace of this \northogonal projection onto $L^{\\perp}$ -- the orthogonal complement of $L$ --\nand is thus equal to the dimension of $L^{\\perp}$, which is $n-p$. \n:::\n\n### Tests and confidence intervals\n\nFor the computation of exact distributional properties of test statistics and \nconfidence intervals we need the strong distributional assumptions A3 and A5. \n\n\\index{$Z$-score!linear model}\n\n::: {#thm-pardisnorm}\nUnder the strong assumptions A3 and A5 it holds, conditionally on $\\mathbf{X}$, that \n$$\n    \\hat{\\beta} \\sim \\mathcal{N}(\\beta,\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1})\n$$\nand that\n$$\n    (n-p)\\hat{\\sigma}^2 \\sim \\sigma^2 \\chi^2_{n-p}.\n$$\nMoreover, for the standardized $Z$-score \n$$\n    Z_j = \\frac{\\hat{\\beta}_j-\\beta_j}{\\hat{\\sigma}\\sqrt{(\\mathbf{X}^T\\mathbf{X})^{-1}_{jj}}} \\sim t_{n-p},\n$$\nor more generally for any $a \\in \\mathbb{R}^{p}$ \n$$\n    Z_a = \\frac{a^T\\hat{\\beta} - a^T \\beta}{\\hat{\\sigma} \\sqrt{a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a}} \\sim t_{n-p}.\n$$\n:::\n\n[The $Z$-score statistics are in this setup also known as the $t$-test statistics. \nUnder the hypothesis $H_0: \\beta_j = 0$ we see that $Z_j = \\hat{\\beta}_j / \\hat{\\mathrm{se}}_j$ \nwhere $\\hat{\\mathrm{se}}_j$ is an estimate of the standard error, that is, the standard deviation\nof the estimator $\\hat{\\beta}_j$.]{.aside}\n\n::: {.proof} \nSee @Hansen:2012, Chapter 10.\n:::\n\nThe standardized $Z$-scores are used to test hypotheses about a single \nparameter or a single linear combination of the parameters. The $Z$-score is \ncomputed under the hypothesis (with the hypothesized value of $\\beta_j$\nor $a^T \\beta$ plugged in), and compared to the $t_{n-p}$ distribution. The test is \ntwo-sided. The $Z$-scores are also used to construct confidence intervals \nfor linear combinations of the parameters. A 95\\% confidence interval for $a^T\\beta$ is computed as \n\\index{confidence interval!linear model}\n$$ \n    a^T\\hat{\\beta} \\pm z_{n-p}  \\hat{\\sigma} \\sqrt{a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a}\n$$ {#eq-confint}\nwhere $\\hat{\\sigma} \\sqrt{a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a}$ is the\nestimated standard error of $a^T\\hat{\\beta}$ and $z_{n-p}$ is the\n$97.5\\%$ quantile in the $t_{n-p}$-distribution. \n\nFor the computation of $a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a$ we do not need to \ncompute the $(\\mathbf{X}^T\\mathbf{X})^{-1}$ \nif we have already computed the QR-decomposition of $\\mathbf{X}$ or the Cholesky decomposition of \n$\\mathbf{X}^T\\mathbf{X}$. With \n$\\mathbf{X}^T\\mathbf{X} = \\mathbf{L} \\mathbf{L}^T$ for a lower triangular[^QR-L]\n$p \\times p$ matrix $\\mathbf{L}$ we find that\n$$\n    \\begin{align*}\n    a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a & = a^T(\\mathbf{L}\\mathbf{L}^T)^{-1}a \\\\\n    & = (\\mathbf{L}^{-1}a)^T \\mathbf{L}^{-1}a \\\\\n    & = b^T b\n    \\end{align*}\n$$\nwhere $b$ solves $\\mathbf{L} b = a$. The solution of this lower triangular system of equations \nis *faster* to compute than the matrix-vector product $(\\mathbf{X}^T\\mathbf{X})^{-1}a$, even\nif the inverse matrix is already computed and stored. This implies\nthat the computation of $(\\mathbf{X}^T\\mathbf{X})^{-1}$ is never computationally beneficial. \nNot even if we need to compute estimated standard errors for many different \nchoices of $a$.  \n\n[^QR-L]: If we have computed the QR-decomposition of $\\mathbf{X}$ we take $\\mathbf{L} = \\mathbf{R}^T$.\n\nTo test hypotheses involving more than a one-dimensional linear combination, we \nneed the $F$-tests. Let $p_0 < p$ and assume that $\\mathbf{X}'$ is an $n \\times p_0$-matrix whose \n$p_0$ columns span a $p_0$-dimensional subspace of the column space of $\\mathbf{X}$. With $\\hat{\\beta}'$\nthe least squares estimator corresponding to $\\mathbf{X}'$ the $F$-test statistic is\ndefined as \\index{$F$-test!linear models}\n$$\n    F = \\frac{\\| \\mathbf{X} \\hat{\\beta} - \\mathbf{X}' \\hat{\\beta}'\\|^2 / (p - p_0)}{\\| \\mathbf{Y} - \\mathbf{X} \\hat{\\beta}\\|^2 / (n - p)}.\n$$ {#eq-F-test-statistic}\nNote that the denominator is just $\\hat{\\sigma}^2$. The $F$-test statistic is one-sided with \nlarge values critical. \n\n::: {#thm-F-test} \nUnder the hypothesis \n$$H_0: \\mathbf{Y} \\mid \\mathbf{X} \\sim \\mathcal{N}(\\mathbf{X}' \\beta_0', \\sigma^2 \\mathbf{I})$$\nthe $F$-test statistic follows an $F$-distribution with $(p - p_0, n - p)$ degrees of freedom. \n:::\n\n::: {.proof}\n See @Hansen:2012, Chapter 10.\n::: \n\nThe terminology associated with the $F$-test and reported by the R function \n`anova()` is as follows (R abbreviations in parentheses). The squared norm \n$\\|\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}\\|^2$\nis called the residual sum of squares (`RSS`) under the model, and $n-p$ is the residual degrees of freedom (`Res. Df`). \nThe squared norm $\\|\\mathbf{X} \\hat{\\beta} - \\mathbf{X}' \\hat{\\beta}'\\|^2$ is the sum of squares \n(`Sum of Sq`), and $p - p_0$ is \nthe degrees of freedom (`Df`). \n\nThe squared norm $\\|\\mathbf{Y} - \\mathbf{X}' \\hat{\\beta}'\\|^2$ is \nthe residual \nsum of squares under the hypothesis, and it follows from Pythagoras's theorem that \n$$\\|\\mathbf{X} \\hat{\\beta} - \\mathbf{X}' \\hat{\\beta}'\\|^2 = \\|\\mathbf{Y} - \\mathbf{X}' \\hat{\\beta}'\\|^2 - \\|\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}\\|^2.$$\nThus the sum of squares is the difference between the residual sum of squares under the hypothesis and \nunder the model. The R function `anova()` computes and reports these numbers together with a \n$p$-value for the null hypothesis derived from the appropriate $F$-distribution. \n\nIt is important for the validity of the $F$-test that the column space, $L'$, of $\\mathbf{X}'$ \nis a subspace of the column space, $L$, of $\\mathbf{X}$. Otherwise the models are not \nnested and a formal hypothesis test is meaningless even if $p_0 < p$. It may not \nbe obvious from the matrices if the models are nested. By definition, $L' \\subseteq L$ if and only \nif \\index{nested models}\n$$\n    \\mathbf{X}' = \\mathbf{X} C \n$$ {#eq-subspace}\nfor a $p \\times p_0$ matrix $C$ (of rank $p_0$), and we can verify that \n$L' \\subseteq L$ by finding a $C$ matrix such that ([-@eq-subspace]) holds. \nThe situation where the columns of $X'$ is a subset of the columns of $X$, which is \na hypothesis on the complementary set of parameters being $0$, \ncorresponds to $C$ having $0$-s or $1$-s appropriately placed \nalong the main diagonal and off-diagonal elements being $0$.\n[It is not necessary to compute the actual $C$ matrix \nif theoretical arguments show that it exists.]{.aside} \n\nIn the ANOVA literature, where categorical predictors\nand their interactions are considered, a considerable amount of work \nhas been invested on choosing the $\\mathbf{X}$ matrix, and thus the parametrization, \nso that relevant scientific hypotheses can be formulated in terms of certain parameters being \n$0$. Though this can ease communication in certain cases, it can also be quite confusing. Linear\nhypotheses considered within the linear model are not specific to a given choice of parametrization,\nand there is no requirement that a linear hypothesis is linked to certain \nparameters being $0$. This will be illustrated in subsequent sections. \n\n## Model assessment {#sec-model-assessment}\n\nThis section introduces methods for investigating deviations from\nthe model assumptions in the data and methods for assessing the descriptive \nor predictive strength of the model. \n\n\\index{residual sum of squares}\nFor the linear model the residual sum of squares \n$$\n    \\mathrm{RSS} = \\|\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}\\|^2\n$$\nquantifies how well $X_i^T \\hat{\\beta}$ predicts $Y_i$ for $i = 1, \\ldots, n$. \nThe absolute value of $\\mathrm{RSS}$ is difficult to interpret. \nThe (in)famous $R^2$ is a first attempt to introduce an interpretable \nindex of predictive strength.\n\n\\index{$R^2$} \\index{coefficient of determination ($R^2$)}\n\n::: {#def-R-square}\nLet $\\mathrm{RSS}_0$ be the residual sum of squares for the intercept only\nmodel and $\\mathrm{RSS}$ the residual sum of squares for the model of interest\n(of dimension $p$). The coefficient of determination is \n$$\n    R^2 = \\frac{\\mathrm{RSS}_0 - \\mathrm{RSS}}{\\mathrm{RSS}_0} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{RSS}_0}.\n$$ {#eq-R-square-def}\n:::\n\n[The $R^2$ quantity has \nsurely been widely overused and over interpreted, but it is a sensible \nquantity to consider, in particular for observational data. Note also the definition \nof the adjusted $R^2$ below.]{.aside}\n\nObserve that the $F$-test statistic for the intercept only model is \n$$\n    F  = \\frac{n - p}{p-1} \\frac{R^2}{1-R^2}\n$$\nThus the coefficient of determination and the $F$-test are in one-to-one correspondence via a \nmonotonely increasing function. The closer $R^2$ is to 1, the larger is the $F$-test.\nWith a single continuous predictor, $R^2$ is the *square of the Pearson\n  correlation* \\index{Pearson correlation!and $R^2$}\n  between the response and the predictor. \nFor this reason, the square root of $R^2$ is called the *multiple correlation\n  coefficient*. \\index{multiple correlation coefficient}\n\nFor two non-nested models with the same complexity (same dimension $p$ in this case),\n  the one with the largest $R^2$ (smallest $\\texttt{RSS}$) is the best predictor\n  on the dataset. But $R^2$ is monotonely increasing in $p$ for \nnested models, which implies that among nested models the more complex model will always predict \nbetter on the data. This does not reflect what will happen if we want to \nmake predictions on an independent observation $(Y^{\\mathrm{new}}, X^{\\mathrm{new}})$. \nThe average residual sum of squares, $\\texttt{RSS}/n$, will generally underestimate the *prediction error*.\n$$\n    E\\left((Y^{\\mathrm{new}} - (X^{\\mathrm{new}})^T \\hat{\\beta})^2 \\mid  \\hat{\\beta} \\right),\n$$\n and the more complex the model is (the larger $p$ is), the more serious is the underestimation. \n It is quite possible that $\\texttt{RSS} \\approx 0$ ($R^2 \\approx 1$) even though the prediction error \n is still considerable. \n\nTo (partly) remedy the optimism of $R^2$ we introduce the *adjusted* $R^2$.\nWe can interpret $1 - R^2$ as a ratio of variance estimates,\n$$\n    1 - R^2 = \\frac{\\mathrm{RSS}/n}{\\mathrm{RSS}_0/n} = \\frac{\\tilde{\\sigma}^2}{\\tilde{\\sigma}_0^2}\n$$\nusing *biased* estimates of the variance. The adjusted $R^2$ is defined as  \n\\index{$R^2$!adjusted}\n$$\n    \\overline{R}^2 = 1 - \\frac{\\mathrm{RSS}/(n-p)}{\\mathrm{RSS}_0/(n-1)} = 1 - (1-R^2) \\frac{n-1}{n-p},\n$$ {#eq-adj-R-square-def}\nwhere the variance ratio is based on *unbiased* estimates of the variance. \n\n\n## Investigating and comparing claim size models {#sec-claim-size-models}\n\nThe lm object `claims_lm` contains a substantial amount of information about the \nfitted linear model -- in addition to the parameter estimates. The generic base \nR function `summary()` can be used to print a summary of that information. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nsummary(claims_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(claims) ~ log(sum), data = claims)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1922 -1.3836 -0.2523  1.2070  8.8644 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.84101    0.29487   19.81   <2e-16 ***\nlog(sum)     0.21149    0.01824   11.59   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.949 on 4034 degrees of freedom\nMultiple R-squared:  0.03225,\tAdjusted R-squared:  0.03201 \nF-statistic: 134.4 on 1 and 4034 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nFrom the summary of the model, including only log insurance sum as a \npredictor, we find that the slope is different from 0 \naccording to the $t$-test. However, $R^2$ (denoted `Multiple R-square` in the \nR output) is only around 3\\% and thus very small, which \nshows that the insurance sum is a quite weak predictor of the claim \nsize. Note that for this regression model with only one continuous predictor,\n$R^2$ is the squared Pearson correlation between log claim size and log insurance sum,\nwhich we can verify directly.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\ncor(log(claims$claims), log(claims$sum))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03224911\n```\n\n\n:::\n:::\n\n\nTwo alternatives to `summary()` are the functions `tidy()` and `glance()` from the broom \npackage. They also extract model information but in a more specific and useful way. \nThe `glance()` function gives a one-line summary of a range of model statistics, \nincluding $R^2$, $\\hat{\\sigma}$ and degrees of freedom.\n\n\n::: {#tbl-claims-lm-glance .cell .column-page-right .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Summary statistics for the simple linear model of claim sizes'}\n\n```{.r .cell-code  code-fold=\"show\"}\nglance(claims_lm) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|   sigma| statistic| p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|-------:|---------:|-------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n| 0.0322491|     0.0320092| 1.94927|  134.4281|       0|  1| -8419.683| 16845.37| 16864.27|  15327.8|        4034| 4036|\n\n\n:::\n:::\n\n\nThe `tidy()` function extracts parameter estimates, standard errors, \ntest statistics and $p$-values, just as `summary()` does, but returns \nthem as a tibble, which is useful for downstream computations. \nThis feature is used below to generate a slightly nicer table via \nthe `kable()` function from the knitr package.\n\n\n::: {#tbl-claims-lm-tidy .cell .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Parameter estimates the simple linear model of claim sizes'}\n\n```{.r .cell-code  code-fold=\"show\"}\ntidy(claims_lm) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|term        |  estimate| std.error| statistic| p.value|\n|:-----------|---------:|---------:|---------:|-------:|\n|(Intercept) | 5.8410060| 0.2948718|  19.80863|       0|\n|log(sum)    | 0.2114886| 0.0182407|  11.59431|       0|\n\n\n:::\n:::\n\n\n\nWe can also consider the parameter estimates for the additive model. \nRecall that we used the sum contrast for the (final version of the) model. \n\n\n::: {#tbl-claims-add-tidy .cell .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Parameter estimates the additive model of claim sizes'}\n\n```{.r .cell-code}\ntidy(claims_lm_add) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|term        |   estimate| std.error|  statistic|   p.value|\n|:-----------|----------:|---------:|----------:|---------:|\n|(Intercept) |  4.0631357| 0.3351685| 12.1226666| 0.0000000|\n|log(sum)    |  0.3300459| 0.0212311| 15.5454313| 0.0000000|\n|grp1        | -0.4657314| 0.0546124| -8.5279353| 0.0000000|\n|grp2        |  0.0815417| 0.0648852|  1.2567073| 0.2089325|\n|grp3        | -0.0643848| 0.0851485| -0.7561478| 0.4496048|\n\n\n:::\n:::\n\n\nThe table shows directly that group 1 has a lower intercept \nthan the common intercept, while it can be inferred from the parametrization that \ngroup 4 has a higher intercept. The effect of \nincluding a trade group specific intercept is also an increased slope compared \nto the previous model. \n\n\n::: {#tbl-claims-add-glance .cell .column-page-right .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Summary statistics for the additive model of claim sizes'}\n\n```{.r .cell-code}\nglance(claims_lm_add) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|    sigma| statistic| p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|--------:|---------:|-------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n|  0.059088|     0.0581544| 1.922765|  63.28537|       0|  4| -8362.926| 16737.85| 16775.67| 14902.71|        4031| 4036|\n\n\n:::\n:::\n\nThe $R^2$ has increased to around 6\\% suggesting that \nthe additive model is better for predictions  than the model without trade group, \nthough it is still far from a strong prediction model. \n\n\n::: {#tbl-claims-int-glance .cell .column-page-right .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Summary statistics for the interaction model of claim sizes'}\n\n```{.r .cell-code}\nglance(claims_lm_int) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|    sigma| statistic| p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|--------:|---------:|-------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n| 0.0606258|     0.0589934| 1.921908|  37.13732|       0|  7| -8359.626| 16737.25| 16793.98| 14878.35|        4028| 4036|\n\n\n:::\n:::\n\n\nFor the interaction model (table above) the $R^2$ has increased a little compared to the additive model, but \n3 additional parameters have also been added to the model. If we look at the adjusted $R^2$ \nthe increase is miniscule. \n\nWe can also compare the models by formally testing one model as a hypothesis within a\nlarger model. This can be done in R with the `anova()` function, which uses the \n$F$-test statistic and the distributional results derived in the \nprevious sections. \n\nTo compare the three models we investigate the two hypotheses \n$$H_0: \\beta_{\\texttt{grp1}} = \\beta_{\\texttt{grp2}} = \\beta_{\\texttt{grp3}} = 0$$\nin the additive model, and \n$$H_1: \\beta_{\\texttt{sum}, \\texttt{grp2}} = \\beta_{\\texttt{sum}, \\texttt{grp3}} = \\beta_{\\texttt{sum}, \\texttt{grp4}} = 0$$\nin the interaction model. These two hypotheses correspond to nested subspaces, and \nthe $F$-tests can be computed by a single call of `anova()`.\n\n\n::: {#tbl-claims-test .cell .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Analysis of variance table comparing the three linear models of claim sizes.'}\n\n```{.r .cell-code  code-fold=\"show\"}\nanova(claims_lm, claims_lm_add, claims_lm_int) |> \n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| Res.Df|      RSS| Df| Sum of Sq|         F|    Pr(>F)|\n|------:|--------:|--:|---------:|---------:|---------:|\n|   4034| 15327.80| NA|        NA|        NA|        NA|\n|   4031| 14902.71|  3| 425.09024| 38.361414| 0.0000000|\n|   4028| 14878.35|  3|  24.35674|  2.198025| 0.0861988|\n\n\n:::\n:::\n\n\nThe conclusion is that we cannot reject $H_1$ while $H_0$ can be clearly rejected. Thus \ndata does not provide evidence that the slope should be different in the four \ntrade groups, but there is strong evidence that the intercept should. This is \nin concordance with the observations made above based on the estimated parameters \nand $R^2$. \n\n## Nonlinear expansions {#sec-nonlinear-expansions}\n\nWe found in the earlier analysis of the insurance claims data a lack of\nmodel fit. This could be due to a nonlinear relation between\n`log(claims)` and `log(sum)`. To handle nonlinear\nrelations between the response and one or more predictors, the predictors\n(as well as the response) can be nonlinearly transformed before they enter\ninto the linear model. Such pre-modeling transformations\nextend the scope of the linear model considerably. For the insurance data\nthe original variables were already log-transformed, and\nit is not obvious which other transformation to use.\n\n\n\n::: {.cell .column-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- seq(11, 21, 0.05)\npol <- as_tibble(cbind(x, poly(x, degree = 5)))\nlong_pol <- tidyr::pivot_longer(pol, cols = !x, names_to = \"basis_fct\")\np <- ggplot(NULL, aes(x, value, colour = basis_fct)) +\n    scale_color_discrete(guide = \"none\") + xlab(\"\") + ylab(\"\")\np + geom_line(data = long_pol, linewidth = 1)\n```\n\n::: {.cell-output-display}\n![A basis of orthogonal polynomials.](2_linear_files/figure-html/fig-poly-1.png){#fig-poly fig-align='center' width=672}\n:::\n:::\n\n\n\nAn alternative to data transformations is to use a small but\nflexible class of *basis functions*, which\ncan capture the nonlinearity. One possibility is to use low degree polynomials.\nThis can be done by simply including powers of a predictor as additional\npredictors.\n[Due to the parsing of formulas in R,\npowers have to be wrapped into an `I()`}-call, e.g., `y ~ x + I(x2)`.]\nDepending on the scale and range of the predictor, this may\nwork just fine. However, raw powers can result in numerical difficulties.\nAn alternative is to use orthogonal polynomials, which are numerically more\nwell behaved.\n\\index{polynomials!orthogonal}\n\n@fig-poly shows an example of an orthogonal basis of\ndegree 5 polynomials on the range 11--21. This corresponds approximately\nto the range of `log(sum)`, which is a target for basis\nexpansion in the example. What should be noted in @fig-poly is\nthat the behavior near the boundary is quite erratic. This is characteristic\nfor polynomial bases. To achieve flexibility in the central part of the range,\nthe polynomials become erratic close to the boundaries. Extrapolation\nbeyond the boundaries cannot be trusted.\n\n\\index{spline}\nAn alternative to polynomials is splines. A spline[^splines]\nis piecewisely a polynomial, and the pieces are joined together in a sufficiently smooth\nway. The points where the polynomials are joined together are called\n*knots*. The flexibility of a spline is determined by the number\nand placement of the knots and the degree of the polynomials. A degree\n$k$ spline is required to be $k-1$ times continuously differentiable.\nA degree 3 spline, also known as a *cubic spline*, is a popular choice,\nwhich thus has a continuous (piecewise linear) second derivative.\n\n[^splines]: A spline is also a thin and flexible wood or metal strip used for smooth curve drawing.\n\n\n::: {.cell .column-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nb_spline <- as_tibble(cbind(x, splines::bs(x, knots = c(14, 18))))\nlong_b_spline <- tidyr::pivot_longer(b_spline, cols = !x, names_to = \"basis_fct\")\np + geom_line(data = long_b_spline, linewidth = 1) +\n    geom_rug(aes(x = c(11, 14, 18, 21), y = NULL, color = NULL), linewidth = 1)\n```\n\n::: {.cell-output-display}\n![A basis of cubic $B$-splines computed using the `bs()` function with two internal knots at 14 and 18 in addition to two boundary knots at 11 and 21.](2_linear_files/figure-html/fig-b-spline-1.png){#fig-b-spline fig-align='center' width=672}\n:::\n:::\n\n\n\\index{spline!cubic}\n@fig-b-spline shows a basis of 5 cubic spline functions.\nThey are so-called $B$-splines \\index{$B$-spline} (basis splines). Note that it is impossible\nto visually detect the knots where the second derivative is\nnon-differentiable. The degree $k$ $B$-spline basis with $r$\ninternal knots has $k + r$ basis functions. The constant function is not included\nthen, and this is precisely what we want when the basis expansion is\nused in a regression model including an intercept. As seen in @fig-b-spline,\nthe $B$-spline basis is also somewhat erratic close to the boundary. For a cubic\nspline, the  behavior close to the boundary can be controlled by requiring that the second and\nthird derivatives are $0$ at the boundary knots. The result is known as a\n*natural cubic spline*. \\index{natural cubic spline} The extrapolation (as a spline) of a\nnatural cubic spline beyond the boundary knots is linear.\n\nDue to the restriction on the derivatives of a natural cubic spline,\nthe basis with $r$ internal knots has $r + 1$ basis functions. Thus the basis for the\nnatural cubic splines with $r + 2$ internal knots has the same number of\nbasis functions as the raw cubic $B$-spline basis with $r$ internal knots. This means in\npractice that, compared to using raw $B$-splines with $r$ internal knots, we can add two\ninternal knots, and thus increase the central flexibility of the model,\nwhile retaining its complexity in terms of $r + 3$ parameters.\n\n\n::: {.cell .column-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nn_spline <- as_tibble(cbind(x, splines::ns(x, knots = c(13, 15, 17, 19))))\nlong_n_spline <- tidyr::pivot_longer(n_spline, cols = !x, names_to = \"basis_fct\")\np + geom_line(data = long_n_spline, linewidth = 1) +\n    geom_rug(aes(x = c(11, 13, 15, 17, 19, 21), y = NULL, color = NULL), linewidth = 1)\n```\n\n::: {.cell-output-display}\n![A $B$-spline basis for natural cubic splines computed using the `ns()` function with internal knots at 13, 15, 17 and 19 in addition to the two boundary knots at 11 and 21.](2_linear_files/figure-html/fig-n-spline-1.png){#fig-n-spline fig-align='center' width=672}\n:::\n:::\n\n\nIt is possible to use orthogonal polynomial expansions as well as spline expansions\nin R together with the `lm()` function via the functions `poly()`,\n`bs()` and `ns()`. The latter two are\nin the splines package, which thus has to be loaded.\n\nWe will illustrate the usage of basis expansions for claim size modeling using\nnatural cubic splines. \\index{spline expansion}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nclaims_lm_spline_add <- lm(\n  log(claims) ~ ns(log(sum), knots = c(13, 15, 17, 19)) + grp,\n  data = claims\n)\nclaims_lm_spline_int <- lm(\n  log(claims) ~ ns(log(sum), knots = c(13, 15, 17, 19)) * grp,\n  data = claims\n)\n```\n:::\n\n\nWhen we fit a model using basis expansions the coefficients of individual\nbasis functions are rarely interpretable or of interest. Thus reporting the\nsummary including all the estimated coefficients is not particularly informative.\nBelow we only extract the last lines from the summary to report the residual\nvariance and the $R^2$ quantities.\n\n\n::: {#tbl-claims-spline-sum1 .cell .column-page-right .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Summary statistics for the additive model based on a spline basis expansion.'}\n\n```{.r .cell-code}\nglance(claims_lm_spline_add) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|  sigma| statistic| p.value| df| logLik|   AIC|   BIC| deviance| df.residual| nobs|\n|---------:|-------------:|------:|---------:|-------:|--:|------:|-----:|-----:|--------:|-----------:|----:|\n|   0.06416|        0.0623| 1.9185|    34.509|       0|  8|  -8352| 16724| 16787|    14822|        4027| 4036|\n\n\n:::\n:::\n\n\n\n::: {#tbl-claims-spline-sum2 .cell .column-page-right .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Summary statistics for the interaction model based on a spline basis expansion.'}\n\n```{.r .cell-code}\nglance(claims_lm_spline_int) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|  sigma| statistic| p.value| df| logLik|   AIC|   BIC| deviance| df.residual| nobs|\n|---------:|-------------:|------:|---------:|-------:|--:|------:|-----:|-----:|--------:|-----------:|----:|\n|   0.07294|       0.06762| 1.9131|    13.724|       0| 23|  -8333| 16716| 16874|    14683|        4012| 4036|\n\n\n:::\n:::\n\n\nCompared to the additive model without the basis expansion, $R^2$ (also the adjusted one)\nis increased a little by the basis expansion. A further increase is observed for the model\nthat includes interactions as well as a basis expansion, and thus gives a unique nonlinear\nrelation between insurance sum and claim size for each trade group. @fig-claims-diag-spline\nshows some diagnostic plots for the additive model, which show that the mean value model is\nnow pretty good, while the residuals are still slightly right skewed. The lack of normality\nis not a problem for using the sampling distributions to draw inference. However, if we compute\nprediction intervals, say, then we must account for the skewness in the residual distribution.\nThis can be done by using quantiles for the empirical distribution of the residuals instead of\nquantiles for the normal distribution.\n\n\n::: {.cell .caption-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nclaims_diag_spline <- augment(claims_lm_spline_add) ## Residuals etc.\ngridExtra::grid.arrange(\n  ggplot(claims_diag_spline, aes(.fitted, .resid), alpha = 0.2) + geom_point() + geom_smooth(),\n  ggplot(claims_diag_spline, aes(.resid)) + geom_histogram(bins = 40),\n  ggplot(claims_diag_spline, aes(sample = .std.resid)) + geom_qq() + geom_abline(),\n  ncol = 3\n)\n```\n\n::: {.cell-output-display}\n![Residual plot (left), histogram (middle) of the raw residuals and qq-plot (right) of the standardized residuals for the additive model with a basis expansion of log insurance sum.](2_linear_files/figure-html/fig-claims-diag-spline-1.png){#fig-claims-diag-spline fig-align='center' width=864}\n:::\n:::\n\n\nGraphical comparisons are typically preferable over parameter comparisons when models involving nonlinear effects and basis expansions are compared. \n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\npred_add <- cbind(\n    claims, \n    exp(predict(claims_lm_spline_add, interval = \"confidence\"))\n)\npred_int <- cbind(\n    claims, \n    exp(predict(claims_lm_spline_int, interval = \"confidence\"))\n)\np0 <- p0 + facet_wrap(~ grp, ncol = 4) + coord_cartesian(ylim = c(100, 1e6))\np0 + geom_line(aes(y = fit), pred_add, color = \"blue\", linewidth = 2) +\n  geom_ribbon(aes(ymax = upr, ymin = lwr), pred_add, fill = \"blue\", alpha = 0.2) +\n  geom_line(aes(y = fit), pred_int, color = \"red\", linewidth = 2) +\n  geom_ribbon(aes(ymax = upr, ymin = lwr), pred_int, fill = \"red\", alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![Scatter plots and fitted values for the claim size models using natural cubic splines including 95% pointwise confidence bands. The additive model (blue) gives translations of the same nonlinear relation for all four trade groups wheras the interaction model (red) gives a separate nonlinear relation for all four trade groups.](2_linear_files/figure-html/fig-spline-fit-1.png){#fig-spline-fit fig-align='center' width=100%}\n:::\n:::\n\n\n@fig-spline-fit shows the fitted models\nstratified according to trade group. The fitted values were computed using the `predict()`\nfunction in R, which can also give upper and lower confidence bounds on the fitted\n values. The fitted value for the $i$-th observation is $X_i^T \\hat{\\beta}$. By \n@thm-pardisnorm a confidence interval for the fitted value can be\ncomputed using @eq-confint with $a = X_i$. The\nconfidence intervals reported by `predict()` for lm objects are computed\nusing this formula, and these are the pointwise confidence bands shown in\n@fig-spline-fit. From this figure any gain of the interaction model\nover the additive model is questionable. The increase of $R^2$, say, appears to be mostly\ndriven by spurious fluctuations. For trade group 4 the capped\nclaims seem overly influential, and because trade group 3 has relatively few observations\nthe nonlinear fit is poorly determined for this group.\n\n\n::: {#tbl-claims-spline-anova .cell .tbl-cap-location-bottom layout-align=\"center\" tbl-cap='Analysis of variance table comparing the two models based on spline expansions and the additive linear model.'}\n\n```{.r .cell-code  code-fold=\"show\"}\nanova(claims_lm_add, claims_lm_spline_add, claims_lm_spline_int) |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| Res.Df|      RSS| Df| Sum of Sq|        F|    Pr(>F)|\n|------:|--------:|--:|---------:|--------:|---------:|\n|   4031| 14902.71| NA|        NA|       NA|        NA|\n|   4027| 14822.42|  4|  80.28605| 5.484225| 0.0002115|\n|   4012| 14683.37| 15| 139.04900| 2.532863| 0.0009413|\n\n\n:::\n:::\n\n\nIn the final comparison above using $F$-tests\nwe first test the hypothesis that the relation is linear in the additive model, and then we\ntest the additive hypothesis when the nonlinear relation is included.\nThe tests formally reject both hypotheses, but the\ngraphical summary in @fig-spline-fit suggests that the nonlinear additive model\nis preferable anyway.\n\nIntroducing a penalty can dampen the spurious fluctuations the arose from\nthe nonlinear basis expansion. We conclude the analysis of the insurance data by \nillustrating how the penalized least squares fit can be computed, and what effect \nthe penalty has on the least squares solution. First recall that the \nleast squares estimator could have been computed by solving the normal equation \ndirectly. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nX <- model.matrix(claims_lm_spline_int)\ny <- model.response(model.frame(claims_lm_spline_int))\nXtX <- crossprod(X)     # Faster than but equivalent to t(X) %*% X\nXty <- crossprod(X, y)  # Faster than but equivalent to t(X) %*% y\ncoef_hat <- solve(XtX, Xty)\n# Comparison\nrange(coef_hat - coefficients(claims_lm_spline_int))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.125585e-09  7.045786e-11\n```\n\n\n:::\n:::\n\n\nThe result obtained by solving the normal equation directly is up to numerical errors \nidentical to the solution computed using `lm()`. The R function `solve()` calls \nthe Fortran routine DGESV from the LAPACK library \nfor solution of linear equations using LU decomposition with partial pivoting (Gaussian \nelimination with row permutations). \n\nLikewise, we can compute the penalized estimator by solving the corresponding linear \nequation. We will do so where we only add a penalty on the coefficients corresponding to\nthe interaction terms in the nonlinear basis expansion. We will use penalty matrices of \nthe form $\\lambda \\boldsymbol{\\Omega}$ for a fixed matrix $\\boldsymbol{\\Omega}$ so \nthat the amount of penalization is controlled by the parameter $\\lambda > 0$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nOmega <- diag(c(rep(0, 9), rep(1, 15)))\ncoef_hat <- cbind(\n    solve(XtX + 0.1 * Omega, Xty),  # Small penalty\n    solve(XtX + Omega, Xty),        # Medium penalty\n    solve(XtX + 10 * Omega, Xty)    # Large penalty\n)  \n```\n:::\n\n\n@fig-penalized shows the resulting penalized model compared to the \nunpenalized interaction model ($\\lambda = 0$) and the additive model ($\\lambda = \\infty$). \nA minimal penalization is enough to\ndampen the most pronounced spurious fluctuation for the fourth trade group \nconsiderably. \n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ntmp <- cbind(\n    coefficients(claims_lm_spline_int),\n    coef_hat, \n    c(coefficients(claims_lm_spline_add), rep(0, 15))\n)\npred <- cbind(claims[, c(\"sum\", \"grp\")], exp(X %*% tmp)) \npred <- tidyr::pivot_longer(pred, cols = !c(grp, sum), names_to = \"lambda\")                                 \np0 + geom_line(\n  aes(y = value, color = lambda), \n  data = pred, \n  linewidth = 1) +\n  theme(legend.position = \"top\") + \n  scale_color_discrete(\n    expression(~lambda), \n    labels = c(0, 0.1, 1, 10, expression(infinity~~~(additive~model)))\n  )     \n```\n\n::: {.cell-output-display}\n![Scatter plots and fitted values for the claim size models where the nonlinear interaction terms have been penalized. The unpenalized interaction and additive model fits are also added.](2_linear_files/figure-html/fig-penalized-1.png){#fig-penalized fig-align='center' width=100%}\n:::\n:::\n\n\nThe penalized least squares estimates can also be computed using the QR decomposition via\nthe function `lm.fit()`. To achieve this we need to augment the model matrix and the \nresponse data suitably. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nfit_lamb1 <- lm.fit(rbind(X, sqrt(0.1) * Omega), c(y, rep(0, ncol(Omega))))\nfit_lamb2 <- lm.fit(rbind(X, sqrt(10) * Omega),  c(y, rep(0, ncol(Omega))))\n## Comparisons\nrange(coef_hat[, 1] - coefficients(fit_lamb1))  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.621947e-11  2.009581e-11\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nrange(coef_hat[, 3] - coefficients(fit_lamb2))  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.994938e-12  6.206813e-12\n```\n\n\n:::\n:::\n\n\nThe results are again (numerically) identical to using `solve()` above. \nIf several penalized estimates using matrices $\\lambda \\boldsymbol{\\Omega}$ for different choices \nof $\\lambda$ are to be computed, a third option using a diagonalization of $\\mathbf{X}^T \\mathbf{X}$ is beneficial. \nThis is implemented in, e.g., the `lm.ridge()` function in the package MASS, which unfortunately \ndoesn't let you choose your own penalty matrix -- it only implements $\\boldsymbol{\\Omega} = I$. \n\nIn conclusion, we have found evidence in the data for an overall nonlinear\nrelation between insurance sum and claim size plus an additive effect of trade group. For small insurance sums (less than DKK 1M) there is almost no relation, while for larger insurance\nsums there is roughly a linear relation on a log-log scale. The model fits the data\nwell with a non-normal and slightly right skewed residual distribution, but as a\npredictive model it is weak, and insurance sum can together with\ntrade group only explain 6-7\\% of the variation in the (log) claim size distribution.\n\n## Exercises {.unnumbered}\n\n::: {#exr-formulas}\nThis exercise covers the relation between the formula interface, used \nto specify a linear model via the `lm()` function, and the model matrix\n$\\mathbf{X}$ that a formula generates internally. The formula interface is convenient\nas it allows us to focus on variables instead of how they are encoded as columns in \n$\\mathbf{X}$. It is, however, useful to know precisely how R interprets the formulas\nand translates them into matrices. The `lm()` function calls the \nfunction `model.matrix()` to create the model matrix from the formula\nand the data. We usually don't call this function ourselves, but understanding \nhow it works gives some insight on how exactly formulas are parsed. \n\nWe simulate an example dataset to work with. @fig-formula-sim-dataexample shows the \nsimulated data.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nset.seed(04022016)\nc <- rep(c(\"A\", \"B\", \"C\"), each = 50)\nformula_data <- tibble(\n    x =  rnorm(150, 0),\n    z = factor(sample(c)),\n    mu = (z == \"A\") + cos(x) + 0.5 * (z == \"B\") * sin(x),\n    y = rnorm(150, mu, 0.5)\n)\n```\n:::\n\n\n\n\n::: {.cell .caption-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The right figure shows a scatter plot of the `a` variable against the `b` variable. The left figure adds color coding according to the group as encoded by the `c` variable. The dashed curves are the true means within the three groups.](2_linear_files/figure-html/fig-formula-sim-dataexample-1.png){#fig-formula-sim-dataexample fig-align='center' width=768}\n:::\n:::\n\n\n:::\n\n1.  Fit a linear regression model using `lm(y ~ x, formula_data)` where `y` is the response variable \n    and `x` is the predictor variable. Investigate the residuals. Does the model fit the data?\n\nThe formula `y ~ x` was used internally for the construction of a model matrix with two \ncolumns. A column of ones representing\nthe intercept, and a column with the observed values of the predictor variable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel.matrix(y ~ x, formula_data) |> \n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)           x\n1           1 -0.00976291\n2           1 -1.64998340\n3           1 -0.32014201\n4           1 -1.42256848\n5           1  1.24188519\n6           1  1.08065770\n```\n\n\n:::\n:::\n\n\n2. How do you get rid of the intercept via the formula specification?\n\nIf we \"add a factor\" on the right hand side of the formula, e.g., \n`y ~ x + z`, the factor's categorical levels will be represented \nvia dummy variable encodings. If it has $k$ levels the model matrix gets $k-1$ \nadditional columns. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel.matrix(y ~ x + z, formula_data) |> \n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)           x zB zC\n1           1 -0.00976291  1  0\n2           1 -1.64998340  1  0\n3           1 -0.32014201  0  0\n4           1 -1.42256848  0  1\n5           1  1.24188519  1  0\n6           1  1.08065770  0  1\n```\n\n\n:::\n:::\n\n\nThe precise encoding is controlled by the `contrasts.arg` argument to `model.matrix()` \n(the `contrasts` argument to `lm()`) or by setting the contrasts associated with \na factor. We can change the base level of `z` to be level 3 (value `C`), say.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncontrasts(formula_data$z) <- \n    contr.treatment(levels(formula_data$z), base = 3)\nmodel.matrix(y ~ x + z, formula_data) |> \n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)           x zA zB\n1           1 -0.00976291  0  1\n2           1 -1.64998340  0  1\n3           1 -0.32014201  1  0\n4           1 -1.42256848  0  0\n5           1  1.24188519  0  1\n6           1  1.08065770  0  0\n```\n\n\n:::\n:::\n\n\nA change of the dummy variable encoding does not change the model (the column space) \nonly the parametrization. This may occasionally be useful for interpretation or for \ntesting certain hypotheses. \n\n3.  How can you check (numerically) that the two model matrices above span\n    the same column space?\n\nIn formulas, the arithmetic operators `*` and `^` together with \n`:` have special meanings. \n\n* Usage of `b * c` is equivalent to `b + c + b : c`.\n* Usage of `b : c` formulates an interaction.\n* Usage of `(a + b + c)^2` is equivalent to `(a + b + c) * (a + b + c)`, which is equivalent to `a + b + c + a : b + a : c + b : c`, etc. \n\nThe following two formulas generate two different model matrices.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nmodel.matrix(y ~ z + x : z, formula_data) |> \n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) zA zB      zA:x        zB:x      zC:x\n1           1  0  1  0.000000 -0.00976291  0.000000\n2           1  0  1  0.000000 -1.64998340  0.000000\n3           1  1  0 -0.320142  0.00000000  0.000000\n4           1  0  0  0.000000  0.00000000 -1.422568\n5           1  0  1  0.000000  1.24188519  0.000000\n6           1  0  0  0.000000  0.00000000  1.080658\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nmodel.matrix(y ~ x * z, formula_data) |>\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)           x zA zB      x:zA        x:zB\n1           1 -0.00976291  0  1  0.000000 -0.00976291\n2           1 -1.64998340  0  1  0.000000 -1.64998340\n3           1 -0.32014201  1  0 -0.320142  0.00000000\n4           1 -1.42256848  0  0  0.000000  0.00000000\n5           1  1.24188519  0  1  0.000000  1.24188519\n6           1  1.08065770  0  0  0.000000  0.00000000\n```\n\n\n:::\n:::\n\n\n4. Are the two models above identical? That is, do the columns of the two \n   model matrices span the same column space?\n\nTransformations of predictor variables can be achieved by formally \napplying an R function to a variable in the formula specification. \nExpansions can be achieved by applying an R function that returns a \nmatrix of basis function evaluations when applied to a vector of \npredictor values. We consider a spline basis expansion.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(splines)\nmodel.matrix(y ~ ns(x, df = 4), formula_data) |>\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) ns(x, df = 4)1 ns(x, df = 4)2 ns(x, df = 4)3 ns(x, df = 4)4\n1           1     0.75361809    0.169569207     0.08775628    -0.04862859\n2           1     0.07603605   -0.191091633     0.44504011    -0.25394848\n3           1     0.77817043    0.005536704     0.13653515    -0.07790959\n4           1     0.14168625   -0.207697339     0.48371373    -0.27601639\n5           1     0.15040919    0.515081509     0.26234462     0.07216468\n6           1     0.20011297    0.527404800     0.24317864     0.02930359\n```\n\n\n:::\n:::\n\n\nBefore you run the R expression below, answer the following question on the \nbasis of what you have learned.\n\n5.  Determine the number of columns for the model matrix below and \n    explain and interpret what the columns mean?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nmodel.matrix(y ~ ns(x, df = 4) * z, formula_data) |>\n    head()\n```\n:::\n\n\n@fig-formula-linear-model was obtained by fitting the linear model using the model \nmatrix, or rather the formula, from above. The figure illustrates the model fit by \ncomparing the fitted model directly to the true means and via the residuals. \nFitted values and residuals were computed using the `augment()` function from the \nbroom package. \n\n\n::: {.cell .caption-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Left: The fitted values plotted as lines together with the data. The dashed lines are the true means. Right: a residual plot.](2_linear_files/figure-html/fig-formula-linear-model-1.png){#fig-formula-linear-model fig-align='center' width=768}\n:::\n:::\n\n\n6. Reproduce @fig-formula-linear-model and discuss the model fit.\n\n::: {#exr-sat}\nThis exercise is based on a dataset relating the mean verbal SAT scores for each of the 50 US states \nto the percentage of students taking the test. The data comes with the RwR package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(RwR)\ndata(sat, package = \"RwR\")\n```\n:::\n\n\n\n::: {.cell .column-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![SAT scores for each US state against percentage of student taking the test.](2_linear_files/figure-html/fig-sat-1.png){#fig-sat fig-align='center' width=384}\n:::\n:::\n\n\n:::\n\n1.  Fit a linear regression model (with an intercept) of the state SAT score as a \n    function of percentage of students. Add the fitted line to the plot.\n\n2.  Fit a piecewise affine function with the \"bend\" at 50% of the \n    students taking the test. How can you formally test the linearity assumption in Question 1 \n    within this larger model?\n\n    *Hint: you want to fit a linear model with a model matrix $\\mathbf{X}$ with\n    3 columns representing an intercept and \n    two piecewise linear functions. You can, for instance, generate a basis using an\n    `ifelse()` construction in the formula. Try different bases. Is there anyone particularly suitable for \n    testing the linearity assumption?*\n\n3.  Fit a natural cubic spline model with 4 knots. Use the `ns()` function to generate a B-spline basis.\n    How should you choose the placements of the knots?\n    How can you formally test the linearity assumption within this larger model?\n\n::: {#exr-column-space}\nShow that if ([-@eq-subspace]) holds then \n$$C = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{X}'.$$\nExplain how this can be used to check if the column space of an \narbitrary $\\mathbf{X}'$ is contained in the column space of $\\mathbf{X}$.\n:::\n\n::: {#exr-column-space-bspline}\nConsider the following matrices\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nX <- splines::bs(\n    seq(0, 1, 0.01), \n    knots = c(0.3, 0.5, 0.7)\n)\nX_prime <- seq(0, 1, 0.01)\n```\n:::\n\n\nof $B$-spline basis functions evaluated on a grid in the interval $[0, 1]$. \nUse the previous exercise to verify that `X_prime` is in the\ncolumn space of `X`. You can use the formula from \nthe exercise to compute $C.$ Can you also use `lm.fit()`?\n:::\n\n::: {#exr-gauss-markov}\nLet $C$ be any $n \\times p$ matrix and consider the *linear estimator* \n$$\n    \\tilde{\\beta} = C^T \\mathbf{Y}.\n$$\n:::\n1.  Show that $\\tilde{\\beta}$ is unbiased as an estimator of $\\beta$ if A1 and A4 hold and if \n   $$\\beta = C^T \\mathbf{X} \\beta$$\n    for all $\\beta$.\n2.  Show that if the estimator is unbiased and A1, A2 and A4 hold then \n    $$V(\\tilde{\\beta} \\mid \\mathbf{X}) = \\sigma^2 C^TC$$\n    and that \n    $$V(\\hat{\\beta}- \\tilde{\\beta} \\mid \\mathbf{X}) = \\sigma^2 (C^TC - (\\mathbf{X}^T\\mathbf{X})^{-1})$$\n    where $\\hat{\\beta}$ is the least squares estimator. \n3.\n    Explain that this shows the Gauss-Markov theorem: Under the assumptions A1 + A2 + A4 the least squares estimator of $\\beta$ has minimal variance among all linear, unbiased estimators of $\\beta$.\n\n\n::: {#exr-empirical-correlation}\nThis exercise relates the empirical correlations between the predictor \nvariables to the variance of the least squares estimator. The\nsetup is as follows. The matrix $\\mathbf{X}$ denotes an $n \\times p$ matrix of\npredictor variables, $\\mathbf{1}$ denotes an $n$ dimensional column vector\nof ones and $(\\mathbf{1} \\ \\ \\mathbf{X})$ denotes the $n \\times (p+1)$ matrix with \nthe first column being $\\mathbf{1}$ and the remaining $n \\times p$ block being \n$\\mathbf{X}$. We let \n$$\\bar{X} = \\frac{1}{n} \\mathbf{X}^T \\mathbf{1}$$\ndenote the $p$ dimensional vector of mean values for the predictor,\nand we let \n$$\\hat{\\Sigma} = \\frac{1}{n}  (\\mathbf{X} - \\mathbf{1} \\bar{X}^T)^T\n(\\mathbf{X} - \\mathbf{1} \\bar{X}^T)$$\ndenote the empirical covariance matrix. It is assumed that $\\hat{\\Sigma}$\nis invertible.\n:::\n1.\n    Show that \n    $$\n        n \\hat{\\Sigma} = \\mathbf{X}^T \\mathbf{X} - n \\bar{X} \\bar{X}^T.\n    $$\n2.\n    Use block matrix inversion to show that \n    $$\n        \\Big((\\mathbf{1} \\ \\ \\mathbf{X})^T (\\mathbf{1} \\ \\ \\mathbf{X})\\Big)^{-1} = \n        \\left(\\begin{array}{cc} * & * \\\\\n        * & \\frac{1}{n} \\hat{\\Sigma}^{-1} \\end{array} \\right).\n    $$\n    Here the $*$-s mean some values that we are not interested in\n    computing in this exercise.\n3.\n    Show that if we fit a \n    linear model with an intercept (which is denoted $\\beta_0$) using the \n    least squares estimator, then the variance \n    of $\\hat{\\beta}_i$, conditionally on $\\mathbf{X}$, for $i = 1,\\ldots, p$ is \n    $$\n        \\frac{\\sigma^2}{n} \\left(\\hat{\\Sigma}^{-1}\\right)_{ii}.\n    $$\n\nWith $\\hat{\\Sigma}_{-1}$ denoting the empirical \ncovariance matrix with the first row and column removed we can write \n$\\hat{\\Sigma}$ in block form as\n$$\n    \\hat{\\Sigma} = \\left( \\begin{array}{cc} \n    \\hat{\\sigma}_1^2 & \\hat{\\gamma}^T \\\\\n    \\hat{\\gamma} & \\hat{\\Sigma}_{-1} \\end{array} \\right).\n$$\n\n4.\n    Use block inversion (again) to show that \n    $$ \n        \\left(\\hat{\\Sigma}^{-1}\\right)_{11} = \\frac{1}{\\hat{\\sigma}_1^2 - \\hat{\\gamma}^T \\left(\\hat{\\Sigma}_{-1}\\right)^{-1} \\hat{\\gamma}}.\n    $$ \n5.\n    Show that in the case $p = 2$, the formula above reduces to \n    $$\n        \\left(\\hat{\\Sigma}^{-1}\\right)_{11} = \\frac{1}{\\hat{\\sigma}_1^2 (1 -\n        \\widehat{\\text{corr}}(X_1, X_2)^2)}.\n    $$\n6.\n    Show that if $\\mathbf{X}_{\\cdot 1} - \\mathbf{1} \\bar{X}_1$ is\n    perpendicular to the remaining columns in \n    $\\mathbf{X} - \\mathbf{1}\\bar{X}^T$, then \n    $$\n        \\left(\\hat{\\Sigma}^{-1}\\right)_{11} = \\frac{1}{\\hat{\\sigma}_1^2}.\n    $$\n7.\n    Interpret the results obtained in this exercise in terms of how the \n    empirical variance of and empirical correlation between predictor variables affect the variances\n    of the estimators $\\hat{\\beta}_i$ for $i = 1, \\ldots, p$. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}